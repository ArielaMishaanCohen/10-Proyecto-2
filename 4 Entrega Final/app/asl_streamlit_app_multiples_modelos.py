"""
Aplicaci√≥n ASL Fingerspelling Recognition
Sistema de clasificaci√≥n y an√°lisis de datos con Streamlit
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
import tensorflow as tf
from tensorflow import keras
import time
import io
import imageio.v2 as imageio


# === Rutas de recursos cargados autom√°ticamente ===
MODEL_PATH = "modelos/my_model.h5"

# === Modelos disponibles (carga autom√°tica) ===
MODEL_PATHS = {
    "Modelo 1": "modelos/my_model.h5",
    "Modelo 2": "modelos/my_model2.h5",
}

CHAR_MAP_PATH = "modelos/character_to_prediction_index.json"

# Conexiones est√°ndar de MediaPipe (0..20)
HAND_CONNECTIONS = [
    (0,1),(1,2),(2,3),(3,4),          # Pulgar
    (0,5),(5,6),(6,7),(7,8),          # √çndice
    (0,9),(9,10),(10,11),(11,12),     # Medio
    (0,13),(13,14),(14,15),(15,16),   # Anular
    (0,17),(17,18),(18,19),(19,20)    # Me√±ique
]


# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="ASL Fingerspelling Recognition",
    page_icon="ü§ü",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Paleta de colores (Teor√≠a del color - Esquema an√°logo con azul como base)
COLORS = {
    'primary': '#2E86AB',      # Azul principal
    'secondary': '#A23B72',    # Magenta oscuro
    'accent': '#F18F01',       # Naranja c√°lido
    'success': '#06A77D',      # Verde esmeralda
    'background': '#F5F5F5',   # Gris claro
    'text': '#2C3E50',         # Gris oscuro
    'card': '#FFFFFF',         # Blanco
    'border': '#E0E0E0'        # Gris borde
}

# CSS personalizado
st.markdown(f"""
<style>
    /* Estilos generales */
    .main {{
        background-color: {COLORS['background']};
    }}
    
    /* T√≠tulos */
    h1 {{
        color: {COLORS['primary']};
        font-weight: 700;
        text-align: center;
        padding: 20px 0;
    }}
    
    h2, h3 {{
        color: {COLORS['text']};
        font-weight: 600;
    }}
    
    /* Tarjetas de m√©tricas */
    .metric-card {{
        background: linear-gradient(135deg, {COLORS['primary']} 0%, {COLORS['secondary']} 100%);
        border-radius: 12px;
        padding: 20px;
        color: white;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin: 10px 0;
    }}
    
    .metric-value {{
        font-size: 2.5em;
        font-weight: bold;
        margin: 10px 0;
    }}
    
    .metric-label {{
        font-size: 1.1em;
        opacity: 0.9;
    }}
    
    /* Botones */
    .stButton > button {{
        background-color: {COLORS['accent']};
        color: white;
        border-radius: 8px;
        padding: 10px 24px;
        font-weight: 600;
        border: none;
        transition: all 0.3s ease;
    }}
    
    .stButton > button:hover {{
        background-color: {COLORS['secondary']};
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }}
    
    /* Sidebar */
    .css-1d391kg {{
        background-color: {COLORS['card']};
    }}
    
    /* Info boxes */
    .info-box {{
        background-color: {COLORS['card']};
        border-left: 4px solid {COLORS['accent']};
        padding: 15px;
        border-radius: 8px;
        margin: 10px 0;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
    }}
    
    /* Alertas */
    .success-box {{
        background-color: #d4edda;
        border-left: 4px solid {COLORS['success']};
        padding: 15px;
        border-radius: 8px;
        color: #155724;
    }}
    
    .warning-box {{
        background-color: #fff3cd;
        border-left: 4px solid {COLORS['accent']};
        padding: 15px;
        border-radius: 8px;
        color: #856404;
    }}
</style>
""", unsafe_allow_html=True)

# Configuraci√≥n de landmarks
LPOSE = [13, 15, 17, 19, 21]
RPOSE = [14, 16, 18, 20, 22]
POSE = LPOSE + RPOSE

X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]
Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]
Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]

SEL_COLS = X + Y + Z

RHAND_IDX = [i for i, col in enumerate(SEL_COLS) if "right" in col]
LHAND_IDX = [i for i, col in enumerate(SEL_COLS) if "left" in col]
RPOSE_IDX = [i for i, col in enumerate(SEL_COLS) if "pose" in col and int(col.split('_')[-1]) in RPOSE]
LPOSE_IDX = [i for i, col in enumerate(SEL_COLS) if "pose" in col and int(col.split('_')[-1]) in LPOSE]

# Configuraci√≥n
FRAME_LEN = 128
TARGET_MAXLEN = 64
PAD_TOKEN = 'P'
START_TOKEN = 'S'
END_TOKEN = 'E'

# ==================== CLASES PERSONALIZADAS DEL MODELO ====================
# Estas clases deben coincidir exactamente con las del entrenamiento

class LandmarkEmbedding(keras.layers.Layer):
    def __init__(self, num_hid=64, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.num_hid = num_hid
        self.dropout_rate = dropout
        self.dense1 = keras.layers.Dense(num_hid, activation="relu")
        self.dropout1 = keras.layers.Dropout(dropout)
        self.dense2 = keras.layers.Dense(num_hid, activation="relu")
        self.dropout2 = keras.layers.Dropout(dropout)

    def call(self, x, training=False):
        if len(x.shape) == 2:
            x = tf.expand_dims(x, 0)
        x = self.dense1(x)
        x = self.dropout1(x, training=training)
        x = self.dense2(x)
        x = self.dropout2(x, training=training)
        seq_len = tf.shape(x)[1]
        positions = tf.range(start=0, limit=seq_len, delta=1, dtype=tf.float32)
        positions = positions[:, tf.newaxis]
        depth = self.num_hid / 2
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        angle_rates = 1 / (10000 ** depths)
        angle_rads = positions * angle_rates
        pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        pos_encoding = pos_encoding[tf.newaxis, :, :]
        return x + pos_encoding

    def get_config(self):
        config = super().get_config()
        config.update({
            "num_hid": self.num_hid,
            "dropout": self.dropout_rate
        })
        return config

class TokenEmbedding(keras.layers.Layer):
    def __init__(self, num_vocab=1000, num_hid=64, **kwargs):
        super().__init__(**kwargs)
        self.num_vocab = num_vocab
        self.num_hid = num_hid
        self.emb = keras.layers.Embedding(num_vocab, num_hid)

    def call(self, x):
        x = self.emb(x)
        seq_len = tf.shape(x)[1]
        positions = tf.range(start=0, limit=seq_len, delta=1, dtype=tf.float32)
        positions = positions[:, tf.newaxis]
        depth = self.num_hid / 2
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        angle_rates = 1 / (10000 ** depths)
        angle_rads = positions * angle_rates
        pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        pos_encoding = pos_encoding[tf.newaxis, :, :]
        return x + pos_encoding

    def get_config(self):
        config = super().get_config()
        config.update({
            "num_vocab": self.num_vocab,
            "num_hid": self.num_hid
        })
        return config

class TransformerEncoder(keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.feed_forward_dim = feed_forward_dim
        self.rate = rate
        self.att = keras.layers.MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=embed_dim // num_heads, 
            dropout=rate
        )
        self.ffn = keras.Sequential([
            keras.layers.Dense(feed_forward_dim, activation="relu"),
            keras.layers.Dropout(rate),
            keras.layers.Dense(embed_dim)
        ])
        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = keras.layers.Dropout(rate)
        self.dropout2 = keras.layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "num_heads": self.num_heads,
            "feed_forward_dim": self.feed_forward_dim,
            "rate": self.rate
        })
        return config

class TransformerDecoder(keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.feed_forward_dim = feed_forward_dim
        self.dropout_rate = dropout_rate
        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.self_att = keras.layers.MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=embed_dim // num_heads, 
            dropout=dropout_rate
        )
        self.enc_att = keras.layers.MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=embed_dim // num_heads, 
            dropout=dropout_rate
        )
        self.self_dropout = keras.layers.Dropout(dropout_rate)
        self.enc_dropout = keras.layers.Dropout(dropout_rate)
        self.ffn_dropout = keras.layers.Dropout(dropout_rate)
        self.ffn = keras.Sequential([
            keras.layers.Dense(feed_forward_dim, activation="relu"),
            keras.layers.Dropout(dropout_rate),
            keras.layers.Dense(embed_dim)
        ])

    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):
        i = tf.range(n_dest)[:, None]
        j = tf.range(n_src)
        m = i >= j - n_src + n_dest
        mask = tf.cast(m, dtype)
        mask = tf.reshape(mask, [1, n_dest, n_src])
        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)
        return tf.tile(mask, mult)

    def call(self, enc_out, target, training):
        input_shape = tf.shape(target)
        batch_size = input_shape[0]
        seq_len = input_shape[1]
        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)
        target_att = self.self_att(target, target, attention_mask=causal_mask, training=training)
        target_norm = self.layernorm1(target + self.self_dropout(target_att, training=training))
        enc_out_att = self.enc_att(target_norm, enc_out, training=training)
        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out_att, training=training) + target_norm)
        ffn_out = self.ffn(enc_out_norm)
        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training=training))
        return ffn_out_norm

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "num_heads": self.num_heads,
            "feed_forward_dim": self.feed_forward_dim,
            "dropout_rate": self.dropout_rate
        })
        return config

class Transformer(keras.Model):
    def __init__(self, num_hid=64, num_head=2, num_feed_forward=128, 
                 num_layers_enc=2, num_layers_dec=1, dropout=0.1, 
                 vocab_size=62, **kwargs):
        super().__init__(**kwargs)
        self.num_hid = num_hid
        self.num_head = num_head
        self.num_feed_forward = num_feed_forward
        self.num_layers_enc = num_layers_enc
        self.num_layers_dec = num_layers_dec
        self.dropout_rate = dropout
        self.vocab_size = vocab_size
        
        self.loss_metric = keras.metrics.Mean(name="loss")
        self.enc_input = LandmarkEmbedding(num_hid=num_hid, dropout=dropout)
        self.dec_input = TokenEmbedding(num_vocab=vocab_size, num_hid=num_hid)
        self.enc_layers = [
            TransformerEncoder(num_hid, num_head, num_feed_forward, rate=dropout) 
            for _ in range(num_layers_enc)
        ]
        self.dec_layers = [
            TransformerDecoder(num_hid, num_head, num_feed_forward, dropout_rate=dropout) 
            for _ in range(num_layers_dec)
        ]
        self.classifier = keras.layers.Dense(vocab_size)

    def call(self, inputs, training=False):
        source = inputs[0]
        target = inputs[1]
        x = self.enc_input(source, training=training)
        for enc_layer in self.enc_layers:
            x = enc_layer(x, training=training)
        enc_out = x
        y = self.dec_input(target)
        for dec_layer in self.dec_layers:
            y = dec_layer(enc_out, y, training=training)
        return self.classifier(y)

    def get_config(self):
        config = super().get_config()
        config.update({
            "num_hid": self.num_hid,
            "num_head": self.num_head,
            "num_feed_forward": self.num_feed_forward,
            "num_layers_enc": self.num_layers_enc,
            "num_layers_dec": self.num_layers_dec,
            "dropout": self.dropout_rate,
            "vocab_size": self.vocab_size
        })
        return config

class TCNBlock(keras.layers.Layer):
    def __init__(self, filters, kernel_size, dilation_rate, dropout_rate, **kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size
        self.dilation_rate = dilation_rate
        self.dropout_rate = dropout_rate
        self.conv = keras.layers.Conv1D(
            filters=filters, 
            kernel_size=kernel_size, 
            padding='causal', 
            dilation_rate=dilation_rate, 
            activation='relu'
        )
        self.dropout = keras.layers.Dropout(dropout_rate)
        self.norm = keras.layers.LayerNormalization()
        
    def call(self, x, training=False):
        out = self.conv(x)
        out = self.dropout(out, training=training)
        out = self.norm(out)
        if x.shape[-1] != out.shape[-1]:
            x = keras.layers.Dense(self.filters)(x)
        out = out + x
        return out

    def get_config(self):
        config = super().get_config()
        config.update({
            "filters": self.filters,
            "kernel_size": self.kernel_size,
            "dilation_rate": self.dilation_rate,
            "dropout_rate": self.dropout_rate
        })
        return config

class TCNEncoder(keras.Model):
    def __init__(self, tcn_filters=64, tcn_kernel_size=3, 
                 tcn_dilations=[1, 2, 4, 8], dropout=0.1, 
                 num_hid=64, **kwargs):
        super().__init__(**kwargs)
        self.tcn_filters = tcn_filters
        self.tcn_kernel_size = tcn_kernel_size
        self.tcn_dilations = tcn_dilations
        self.dropout_rate = dropout
        self.num_hid = num_hid
        
        self.input_proj = keras.layers.Dense(tcn_filters)
        self.tcn_blocks = [
            TCNBlock(tcn_filters, tcn_kernel_size, dilation, dropout) 
            for dilation in tcn_dilations
        ]
        self.output_proj = keras.layers.Dense(num_hid)
        
    def call(self, x, training=False):
        x = self.input_proj(x)
        for block in self.tcn_blocks:
            x = block(x, training=training)
        x = self.output_proj(x)
        return x

    def get_config(self):
        config = super().get_config()
        config.update({
            "tcn_filters": self.tcn_filters,
            "tcn_kernel_size": self.tcn_kernel_size,
            "tcn_dilations": self.tcn_dilations,
            "dropout": self.dropout_rate,
            "num_hid": self.num_hid
        })
        return config

class TCNModel(keras.Model):
    def __init__(self, tcn_filters=64, tcn_kernel_size=3, 
                 tcn_dilations=[1, 2, 4, 8], dropout=0.1,
                 num_hid=64, num_head=2, num_feed_forward=128,
                 num_layers_dec=1, vocab_size=62, **kwargs):
        super().__init__(**kwargs)
        self.tcn_filters = tcn_filters
        self.tcn_kernel_size = tcn_kernel_size
        self.tcn_dilations = tcn_dilations
        self.dropout_rate = dropout
        self.num_hid = num_hid
        self.num_head = num_head
        self.num_feed_forward = num_feed_forward
        self.num_layers_dec = num_layers_dec
        self.vocab_size = vocab_size
        
        self.loss_metric = keras.metrics.Mean(name="loss")
        self.encoder = TCNEncoder(tcn_filters, tcn_kernel_size, tcn_dilations, dropout, num_hid)
        self.dec_input = TokenEmbedding(num_vocab=vocab_size, num_hid=num_hid)
        self.dec_layers = [
            TransformerDecoder(num_hid, num_head, num_feed_forward, dropout_rate=dropout) 
            for _ in range(num_layers_dec)
        ]
        self.classifier = keras.layers.Dense(vocab_size)
    
    def call(self, inputs, training=False):
        source = inputs[0]
        target = inputs[1]
        enc_out = self.encoder(source, training=training)
        y = self.dec_input(target)
        for dec_layer in self.dec_layers:
            y = dec_layer(enc_out, y, training=training)
        return self.classifier(y)

    def get_config(self):
        config = super().get_config()
        config.update({
            "tcn_filters": self.tcn_filters,
            "tcn_kernel_size": self.tcn_kernel_size,
            "tcn_dilations": self.tcn_dilations,
            "dropout": self.dropout_rate,
            "num_hid": self.num_hid,
            "num_head": self.num_head,
            "num_feed_forward": self.num_feed_forward,
            "num_layers_dec": self.num_layers_dec,
            "vocab_size": self.vocab_size
        })
        return config

# ==================== FIN CLASES PERSONALIZADAS ====================

# Funciones auxiliares
@st.cache_resource
def load_model(model_path):
    """Carga el modelo entrenado con capas personalizadas"""
    try:
        import h5py
        import json
        
        st.info("üîÑ Cargando modelo...")
        
        # Leer el archivo H5 para inspeccionar su estructura
        with h5py.File(model_path, 'r') as f:
            # Primero, intentar inferir desde los PESOS (m√°s confiable que el config)
            num_hid = None
            num_head = None
            vocab_size = 62
            
            if 'model_weights' in f:
                weight_group = f['model_weights']
                
                # Buscar LandmarkEmbedding para obtener num_hid
                for layer_name in weight_group.keys():
                    if 'landmark_embedding' in layer_name or 'token_embedding' in layer_name:
                        layer_group = weight_group[layer_name]
                        weight_names = [n.decode('utf8') if isinstance(n, bytes) else n 
                                       for n in layer_group.attrs.get('weight_names', [])]
                        
                        for weight_name in weight_names:
                            if 'kernel' in weight_name or 'embeddings' in weight_name:
                                if weight_name in layer_group:
                                    weight_shape = layer_group[weight_name].shape
                                    # Para embedding: shape es (input_dim, num_hid) o (vocab, num_hid)
                                    if len(weight_shape) == 2:
                                        num_hid = weight_shape[1]
                                        if 'token_embedding' in layer_name:
                                            vocab_size = weight_shape[0]
                                        st.info(f"üîç Detectado desde {layer_name}: num_hid={num_hid}")
                                        break
                        if num_hid:
                            break
                
                # Buscar TransformerEncoder para obtener num_head
                for layer_name in weight_group.keys():
                    if 'transformer_encoder' in layer_name:
                        layer_group = weight_group[layer_name]
                        
                        # Buscar pesos de atenci√≥n multi-head
                        for sub_layer in layer_group.keys():
                            if 'multi_head_attention' in sub_layer:
                                mha_group = layer_group[sub_layer]
                                
                                for weight_name in mha_group.keys():
                                    if 'query' in weight_name and 'kernel' in weight_name:
                                        weight_shape = mha_group[weight_name].shape
                                        # Shape deber√≠a ser (num_hid, num_head, key_dim)
                                        if len(weight_shape) == 3:
                                            num_head = weight_shape[1]
                                            st.info(f"üîç Detectado desde {layer_name}: num_head={num_head}")
                                            break
                                if num_head:
                                    break
                        if num_head:
                            break
            
            # Si no se pudo inferir, intentar desde config
            if num_hid is None or num_head is None:
                if 'model_config' in f.attrs:
                    model_config_str = f.attrs['model_config']
                    if isinstance(model_config_str, bytes):
                        model_config_str = model_config_str.decode('utf-8')
                    model_config = json.loads(model_config_str)
                    
                    config = model_config.get('config', {})
                    if num_hid is None:
                        num_hid = config.get('num_hid', 128)
                    if num_head is None:
                        num_head = config.get('num_head', 4)
                    vocab_size = config.get('vocab_size', vocab_size)
                    model_class = model_config.get('class_name', 'Transformer')
                else:
                    # Valores por defecto basados en lo observado
                    if num_hid is None:
                        num_hid = 128
                    if num_head is None:
                        num_head = 4
                    model_class = 'Transformer'
            else:
                model_class = 'Transformer'
            
            # Calcular otros par√°metros basados en los detectados
            num_feed_forward = num_hid * 2
            num_layers_enc = 2
            num_layers_dec = 1
            dropout = 0.0
            
            st.info(f"üìã Tipo: {model_class}")
            st.info(f"üìä Par√°metros FINALES: vocab={vocab_size}, hid={num_hid}, heads={num_head}, ff={num_feed_forward}")
        
        # Registrar objetos personalizados
        custom_objects = {
            'LandmarkEmbedding': LandmarkEmbedding,
            'TokenEmbedding': TokenEmbedding,
            'TransformerEncoder': TransformerEncoder,
            'TransformerDecoder': TransformerDecoder,
            'Transformer': Transformer,
            'TCNBlock': TCNBlock,
            'TCNEncoder': TCNEncoder,
            'TCNModel': TCNModel
        }
        
        # Intentar carga directa con custom_object_scope
        st.info("üîß M√©todo 1: Carga directa...")
        try:
            with keras.utils.custom_object_scope(custom_objects):
                from tensorflow import keras as tf_keras
                model = tf_keras.models.load_model(model_path, compile=False)
                
            st.success("‚úÖ Modelo cargado exitosamente!")
            return model
            
        except Exception as e1:
            st.warning(f"M√©todo 1 fall√≥: {str(e1)[:150]}")
            
            # M√©todo 2: Reconstrucci√≥n completa con par√°metros correctos
            st.info("üîß M√©todo 2: Reconstrucci√≥n con par√°metros detectados...")
            try:
                # Crear modelo con los par√°metros correctos
                if model_class == 'Transformer':
                    model = Transformer(
                        num_hid=num_hid,
                        num_head=num_head,
                        num_feed_forward=num_feed_forward,
                        num_layers_enc=num_layers_enc,
                        num_layers_dec=num_layers_dec,
                        dropout=dropout,
                        vocab_size=vocab_size
                    )
                elif model_class == 'TCNModel':
                    model = TCNModel(
                        tcn_filters=config.get('tcn_filters', 64),
                        tcn_kernel_size=config.get('tcn_kernel_size', 3),
                        tcn_dilations=config.get('tcn_dilations', [1, 2, 4, 8]),
                        dropout=dropout,
                        num_hid=num_hid,
                        num_head=num_head,
                        num_feed_forward=num_feed_forward,
                        num_layers_dec=num_layers_dec,
                        vocab_size=vocab_size
                    )
                
                # Construir el modelo con datos dummy
                dummy_source = tf.ones((1, FRAME_LEN, 78))
                dummy_target = tf.ones((1, TARGET_MAXLEN), dtype=tf.int32)
                _ = model([dummy_source, dummy_target], training=False)
                
                st.info(f"üèóÔ∏è Modelo construido con {len(model.layers)} capas")
                
                # Cargar pesos capa por capa
                with h5py.File(model_path, 'r') as f:
                    if 'model_weights' in f:
                        weight_group = f['model_weights']
                        
                        saved_layers = list(weight_group.keys())
                        st.info(f"üíæ Capas guardadas: {len(saved_layers)}")
                        
                        layer_names = [layer.name for layer in model.layers]
                        st.info(f"üîç Capas del modelo: {layer_names}")
                        
                        # Cargar pesos para cada capa
                        loaded_count = 0
                        skipped_layers = []
                        
                        for layer in model.layers:
                            if layer.name in weight_group:
                                try:
                                    layer_group = weight_group[layer.name]
                                    weight_names = [n.decode('utf8') if isinstance(n, bytes) else n 
                                                   for n in layer_group.attrs['weight_names']]
                                    
                                    weight_values = []
                                    for weight_name in weight_names:
                                        if weight_name in layer_group:
                                            weight_values.append(layer_group[weight_name][()])
                                    
                                    if weight_values:
                                        # Verificar compatibilidad de formas
                                        current_weights = layer.get_weights()
                                        if len(weight_values) == len(current_weights):
                                            compatible = True
                                            for i, (saved_w, current_w) in enumerate(zip(weight_values, current_weights)):
                                                if saved_w.shape != current_w.shape:
                                                    compatible = False
                                                    st.warning(f"‚ö†Ô∏è Forma incompatible en {layer.name}[{i}]: {saved_w.shape} vs {current_w.shape}")
                                                    break
                                            
                                            if compatible:
                                                layer.set_weights(weight_values)
                                                loaded_count += 1
                                            else:
                                                skipped_layers.append(layer.name)
                                        else:
                                            skipped_layers.append(layer.name)
                                            st.warning(f"‚ö†Ô∏è N√∫mero de pesos diferente en {layer.name}")
                                except Exception as layer_error:
                                    skipped_layers.append(layer.name)
                                    st.warning(f"‚ö†Ô∏è Error cargando {layer.name}: {str(layer_error)[:100]}")
                        
                        st.success(f"‚úÖ Cargados pesos de {loaded_count}/{len(model.layers)} capas")
                        
                        if skipped_layers:
                            st.warning(f"‚ö†Ô∏è Capas omitidas: {skipped_layers}")
                
                if loaded_count > 0:
                    st.success("‚úÖ Modelo cargado con m√©todo 2!")
                    st.info(f"üí° {loaded_count} de {len(model.layers)} capas cargadas correctamente")
                    return model
                else:
                    raise ValueError("No se pudieron cargar pesos")
                    
            except Exception as e2:
                st.error(f"‚ùå M√©todo 2 fall√≥: {str(e2)}")
                
                # M√©todo 3: Carga forzada con skip_mismatch
                st.info("üîß M√©todo 3: Carga forzada...")
                try:
                    import warnings
                    warnings.filterwarnings('ignore')
                    
                    model = Transformer(
                        num_hid=num_hid,
                        num_head=num_head,
                        num_feed_forward=num_feed_forward,
                        num_layers_enc=num_layers_enc,
                        num_layers_dec=num_layers_dec,
                        dropout=dropout,
                        vocab_size=vocab_size
                    )
                    
                    dummy_source = tf.ones((1, FRAME_LEN, 78))
                    dummy_target = tf.ones((1, TARGET_MAXLEN), dtype=tf.int32)
                    _ = model([dummy_source, dummy_target], training=False)
                    
                    # Cargar con skip_mismatch
                    model.load_weights(model_path, skip_mismatch=True, by_name=True)
                    st.warning("‚ö†Ô∏è Modelo cargado parcialmente con skip_mismatch")
                    return model
                        
                except Exception as e3:
                    st.error(f"‚ùå Todos los m√©todos fallaron")
                    with st.expander("üîç Ver detalles del error"):
                        st.text(f"Error m√©todo 1: {str(e1)}")
                        st.text(f"Error m√©todo 2: {str(e2)}")
                        st.text(f"Error m√©todo 3: {str(e3)}")
                    return None
        
    except Exception as e:
        st.error(f"‚ùå Error cr√≠tico al cargar el modelo: {str(e)}")
        import traceback
        with st.expander("üîç Ver traceback completo"):
            st.code(traceback.format_exc())
        return None

@st.cache_data
def load_char_map(char_map_path):
    """Carga el mapeo de caracteres"""
    try:
        with open(char_map_path, 'r') as f:
            char_to_num = json.load(f)
        
        # Agregar tokens especiales
        char_to_num[PAD_TOKEN] = max(char_to_num.values()) + 1
        char_to_num[START_TOKEN] = max(char_to_num.values()) + 1
        char_to_num[END_TOKEN] = max(char_to_num.values()) + 1
        
        num_to_char = {j: i for i, j in char_to_num.items()}
        return char_to_num, num_to_char
    except Exception as e:
        st.error(f"Error al cargar el mapeo de caracteres: {str(e)}")
        return None, None

def resize_pad(x):
    """Redimensiona y rellena secuencias"""
    if tf.shape(x)[0] < FRAME_LEN:
        x = tf.pad(x, ([[0, FRAME_LEN - tf.shape(x)[0]], [0, 0], [0, 0]]))
    else:
        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))
    return x

def pre_process(x):
    """Preprocesa los landmarks"""
    if not isinstance(x, tf.Tensor):
        x = tf.constant(x, dtype=tf.float32)
    
    # Extraer manos y poses
    rhand = tf.gather(x, RHAND_IDX, axis=1)
    lhand = tf.gather(x, LHAND_IDX, axis=1)
    rpose = tf.gather(x, RPOSE_IDX, axis=1)
    lpose = tf.gather(x, LPOSE_IDX, axis=1)
    
    # Determinar qu√© mano usar
    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)
    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)
    rnans = tf.math.count_nonzero(rnan_idx)
    lnans = tf.math.count_nonzero(lnan_idx)
    
    if rnans > lnans:
        hand = lhand
        pose = lpose
        # Invertir x para mano izquierda
        hand_len = len(LHAND_IDX) // 3
        hand_x = hand[:, 0*hand_len:1*hand_len]
        hand_y = hand[:, 1*hand_len:2*hand_len]
        hand_z = hand[:, 2*hand_len:3*hand_len]
        hand = tf.concat([1.0 - hand_x, hand_y, hand_z], axis=1)
        
        pose_len = len(LPOSE_IDX) // 3
        pose_x = pose[:, 0*pose_len:1*pose_len]
        pose_y = pose[:, 1*pose_len:2*pose_len]
        pose_z = pose[:, 2*pose_len:3*pose_len]
        pose = tf.concat([1.0 - pose_x, pose_y, pose_z], axis=1)
    else:
        hand = rhand
        pose = rpose
    
    # Normalizar mano
    hand_len = len(LHAND_IDX) // 3
    hand_x = hand[:, 0*hand_len:1*hand_len]
    hand_y = hand[:, 1*hand_len:2*hand_len]
    hand_z = hand[:, 2*hand_len:3*hand_len]
    hand = tf.stack([hand_x, hand_y, hand_z], axis=-1)
    
    mean = tf.math.reduce_mean(hand, axis=1, keepdims=True)
    std = tf.math.reduce_std(hand, axis=1, keepdims=True)
    hand = (hand - mean) / (std + 1e-8)
    
    # Normalizar pose
    pose_len = len(LPOSE_IDX) // 3
    pose_x = pose[:, 0*pose_len:1*pose_len]
    pose_y = pose[:, 1*pose_len:2*pose_len]
    pose_z = pose[:, 2*pose_len:3*pose_len]
    pose = tf.stack([pose_x, pose_y, pose_z], axis=-1)
    
    # Concatenar
    x = tf.concat([hand, pose], axis=1)
    x = resize_pad(x)
    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)
    x = tf.reshape(x, (FRAME_LEN, -1))
    
    return x

def load_parquet_data(file_path):
    """Carga datos de un archivo parquet"""
    try:
        df = pd.read_parquet(file_path, columns=SEL_COLS)
        return df
    except Exception as e:
        st.error(f"Error al cargar archivo: {str(e)}")
        return None

def predict_sequence(model, landmarks, char_to_num, num_to_char, temperature=0.8):
    """Realiza predicci√≥n en una secuencia de landmarks"""
    try:
        # Preprocesar
        processed = pre_process(landmarks.values.astype(np.float32))
        processed = tf.expand_dims(processed, 0)  # Batch dimension
        
        # Generar predicci√≥n
        start_token = char_to_num[START_TOKEN]
        dec_input = tf.ones((1, 1), dtype=tf.int32) * start_token
        
        for i in range(TARGET_MAXLEN - 1):
            # Forward pass
            predictions = model([processed, dec_input], training=False)
            
            # Obtener siguiente token
            logits = predictions[:, -1, :] / temperature
            probabilities = tf.nn.softmax(logits, axis=-1)
            next_token = tf.argmax(probabilities, axis=-1, output_type=tf.int32)
            next_token = tf.expand_dims(next_token, -1)
            
            # Agregar a secuencia
            dec_input = tf.concat([dec_input, next_token], axis=-1)
            
            # Verificar END_TOKEN
            if next_token[0, 0].numpy() == char_to_num[END_TOKEN]:
                break
        
        # Decodificar
        tokens = dec_input[0].numpy()
        result = []
        for token in tokens:
            char = num_to_char[token]
            if char == END_TOKEN:
                break
            if char not in [START_TOKEN, PAD_TOKEN]:
                result.append(char)
        
        return ''.join(result)
    except Exception as e:
        st.error(f"Error en predicci√≥n: {str(e)}")
        return None

def create_landmark_visualization(df):
    """Crea visualizaci√≥n 3D de landmarks"""
    # Tomar primer frame
    frame_0 = df.iloc[0]
    
    # Extraer coordenadas
    x_coords = [frame_0[col] for col in X]
    y_coords = [frame_0[col] for col in Y]
    z_coords = [frame_0[col] for col in Z]
    
    # Crear figura 3D
    fig = go.Figure()
    
    # Mano derecha
    rhand_x = x_coords[:21]
    rhand_y = y_coords[:21]
    rhand_z = z_coords[:21]
    
    fig.add_trace(go.Scatter3d(
        x=rhand_x, y=rhand_y, z=rhand_z,
        mode='markers+lines',
        name='Mano Derecha',
        marker=dict(size=5, color=COLORS['primary']),
        line=dict(color=COLORS['primary'], width=2)
    ))
    
    # Mano izquierda
    lhand_x = x_coords[21:42]
    lhand_y = y_coords[21:42]
    lhand_z = z_coords[21:42]
    
    fig.add_trace(go.Scatter3d(
        x=lhand_x, y=lhand_y, z=lhand_z,
        mode='markers+lines',
        name='Mano Izquierda',
        marker=dict(size=5, color=COLORS['accent']),
        line=dict(color=COLORS['accent'], width=2)
    ))
    
    # Pose
    pose_x = x_coords[42:]
    pose_y = y_coords[42:]
    pose_z = z_coords[42:]
    
    fig.add_trace(go.Scatter3d(
        x=pose_x, y=pose_y, z=pose_z,
        mode='markers',
        name='Pose',
        marker=dict(size=8, color=COLORS['success'])
    ))
    
    fig.update_layout(
        title='Visualizaci√≥n de Landmarks (Frame 0)',
        scene=dict(
            xaxis_title='X',
            yaxis_title='Y',
            zaxis_title='Z',
            bgcolor=COLORS['background']
        ),
        paper_bgcolor=COLORS['card'],
        font=dict(color=COLORS['text']),
        height=600
    )
    
    return fig

def create_temporal_analysis(df):
    """Crea an√°lisis temporal de velocidad 3D de la mano derecha (robusto a NaN)."""

    if len(df) < 2:
        fig = go.Figure()
        fig.update_layout(
            title='An√°lisis Temporal: (insuficientes frames)',
            paper_bgcolor=COLORS['card'],
            plot_bgcolor=COLORS['background'],
            font=dict(color=COLORS['text']),
            height=400
        )
        return fig

    # Extraer mano derecha completa
    Rx = df[X[:21]].to_numpy(dtype="float32")
    Ry = df[Y[:21]].to_numpy(dtype="float32")
    Rz = df[Z[:21]].to_numpy(dtype="float32")

    # Reemplazar NaN por 0 (o puedes usar forward-fill si prefieres)
    Rx = np.nan_to_num(Rx, nan=0.0)
    Ry = np.nan_to_num(Ry, nan=0.0)
    Rz = np.nan_to_num(Rz, nan=0.0)

    # Diferencias entre frames (t, 21) -> (t-1, 21)
    dRx = np.diff(Rx, axis=0)
    dRy = np.diff(Ry, axis=0)
    dRz = np.diff(Rz, axis=0)

    # Velocidad por frame: norma L2 por landmark y sumar, o norma global:
    # aqu√≠ sumamos las normas por landmark => escalar por frame
    # vel[t] = sqrt( sum_i (dRx^2 + dRy^2 + dRz^2)_i )
    sq = dRx**2 + dRy**2 + dRz**2
    vel = np.sqrt(np.sum(sq, axis=1))  # shape (t-1,)

    # Si todo qued√≥ en cero (p.ej. coords constantes), mostramos de todas formas
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=np.arange(1, len(df)),  # vel est√° desplazada una posici√≥n
        y=vel,
        mode='lines',
        name='Velocidad',
        line=dict(color=COLORS['primary'], width=2),
        fill='tozeroy'
    ))
    fig.update_layout(
        title='An√°lisis Temporal: Velocidad de Movimiento (mano derecha)',
        xaxis_title='Frame',
        yaxis_title='Velocidad',
        paper_bgcolor=COLORS['card'],
        plot_bgcolor=COLORS['background'],
        font=dict(color=COLORS['text']),
        height=400
    )
    return fig

def create_coordinate_distribution(df):
    """Crea distribuci√≥n de coordenadas"""
    # Obtener todas las coordenadas X
    all_x = df[X].values.flatten()
    all_x = all_x[~np.isnan(all_x)]
    
    # Crear histograma
    fig = go.Figure()
    
    fig.add_trace(go.Histogram(
        x=all_x,
        nbinsx=50,
        name='Coordenadas X',
        marker_color=COLORS['secondary']
    ))
    
    fig.update_layout(
        title='Distribuci√≥n de Coordenadas X',
        xaxis_title='Valor de X',
        yaxis_title='Frecuencia',
        paper_bgcolor=COLORS['card'],
        plot_bgcolor=COLORS['background'],
        font=dict(color=COLORS['text']),
        height=400
    )
    
    return fig

def create_correlation_heatmap(df):
    """Crea mapa de calor de correlaciones"""
    # Seleccionar algunas caracter√≠sticas clave
    sample_cols = X[:10] + Y[:10]  # Primeros 10 landmarks de cada eje
    
    # Calcular correlaciones
    corr_matrix = df[sample_cols].corr()
    
    # Crear heatmap
    fig = go.Figure(data=go.Heatmap(
        z=corr_matrix.values,
        x=[col.replace('x_', '').replace('y_', '') for col in sample_cols],
        y=[col.replace('x_', '').replace('y_', '') for col in sample_cols],
        colorscale='RdBu',
        zmid=0
    ))
    
    fig.update_layout(
        title='Correlaci√≥n entre Landmarks',
        paper_bgcolor=COLORS['card'],
        font=dict(color=COLORS['text']),
        height=500
    )
    
    return fig

# ===== GIF 3D de landmarks (toda la secuencia) =====
def _hex_to_rgb(h):
    h = h.lstrip('#')
    return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))

def create_landmark_gif(df, fps=10, width=800, height=700, max_frames=300):
    """
    Genera un GIF (bytes) recorriendo todos los frames.
    Requiere: pip install kaleido imageio
    - fps: cuadros por segundo del GIF
    - width/height: tama√±o del render
    - max_frames: limita frames para evitar GIFs gigantes (ajusta si quieres)
    """
    # Determinar cu√°ntos frames hay (las filas del DF son frames)
    n_frames = len(df)
    if n_frames < 1:
        return None

    # Limitar para evitar archivos enormes
    n_use = min(n_frames, max_frames)

    # Precomputar rangos fijos para que el GIF no ‚Äúsalte‚Äù
    # (si tus coords est√°n [0,1], puedes fijar manualmente estos rangos)
    def _global_range(cols):
        vals = df[cols].to_numpy().astype("float32")
        vals = vals[~np.isnan(vals)]
        if vals.size == 0:
            return (0.0, 1.0)
        lo, hi = float(vals.min()), float(vals.max())
        if lo == hi:
            lo -= 0.5
            hi += 0.5
        margin = 0.05 * (hi - lo)
        return (lo - margin, hi + margin)

    x_min, x_max = _global_range(X)
    y_min, y_max = _global_range(Y)
    z_min, z_max = _global_range(Z)

    # Colores (tu paleta)
    c_r = COLORS['primary']
    c_l = COLORS['accent']
    c_p = COLORS['success']

    # Para exportar im√°genes con kaleido:
    # pip install kaleido
    frames_imgs = []

    for i in range(n_use):
        row = df.iloc[i]

        # Extraer coords por frame (manejar NaN -> 0)
        def _safe_vals(cols):
            arr = row[cols].to_numpy(dtype="float32")
            arr = np.nan_to_num(arr, nan=0.0)
            return arr

        Rx, Ry, Rz = _safe_vals(X[:21]), _safe_vals(Y[:21]), _safe_vals(Z[:21])
        Lx, Ly, Lz = _safe_vals(X[21:42]), _safe_vals(Y[21:42]), _safe_vals(Z[21:42])
        Px, Py, Pz = _safe_vals(X[42:]), _safe_vals(Y[42:]), _safe_vals(Z[42:])

        fig = go.Figure()

        fig.add_trace(go.Scatter3d(
            x=Rx, y=Ry, z=Rz,
            mode='markers+lines',
            name='Mano Derecha',
            marker=dict(size=5, color=c_r),
            line=dict(color=c_r, width=2)
        ))
        fig.add_trace(go.Scatter3d(
            x=Lx, y=Ly, z=Lz,
            mode='markers+lines',
            name='Mano Izquierda',
            marker=dict(size=5, color=c_l),
            line=dict(color=c_l, width=2)
        ))
        fig.add_trace(go.Scatter3d(
            x=Px, y=Py, z=Pz,
            mode='markers',
            name='Pose',
            marker=dict(size=8, color=c_p)
        ))

        fig.update_layout(
            title=f'Landmarks ‚Äî Frame {i+1}/{n_use}',
            scene=dict(
                xaxis_title='X', xaxis=dict(range=[x_min, x_max]),
                yaxis_title='Y', yaxis=dict(range=[y_min, y_max]),
                zaxis_title='Z', zaxis=dict(range=[z_min, z_max]),
                bgcolor=COLORS['background']
            ),
            paper_bgcolor=COLORS['card'],
            font=dict(color=COLORS['text']),
            width=width,
            height=height,
            margin=dict(l=0, r=0, b=0, t=50)
        )

        # Render est√°tico a PNG (bytes) con kaleido
        png_bytes = fig.to_image(format="png", width=width, height=height, scale=1)
        frames_imgs.append(imageio.imread(png_bytes))

    # Escribir GIF en memoria
    buf = io.BytesIO()
    imageio.mimsave(buf, frames_imgs, format="GIF", duration=1.0/float(fps))
    buf.seek(0)
    return buf.read()

def create_landmark_animation_3d(df, step=2, max_frames=300):

    n = len(df)
    if n < 1: 
        return go.Figure()

    idxs = list(range(0, min(n, max_frames), step))

    def _frame_vals(i):
        row = df.iloc[i]
        def _safe(cols):
            arr = row[cols].to_numpy(dtype="float32")
            return np.nan_to_num(arr, nan=0.0)
        Rx, Ry, Rz = _safe([f'x_right_hand_{k}' for k in range(21)]), \
                     _safe([f'y_right_hand_{k}' for k in range(21)]), \
                     _safe([f'z_right_hand_{k}' for k in range(21)])
        Lx, Ly, Lz = _safe([f'x_left_hand_{k}'  for k in range(21)]), \
                     _safe([f'y_left_hand_{k}'  for k in range(21)]), \
                     _safe([f'z_left_hand_{k}'  for k in range(21)])
        return (Rx, Ry, Rz, Lx, Ly, Lz)

    # Rango fijo
    def _rng(cols):
        vals = df[cols].to_numpy(dtype="float32")
        vals = vals[~np.isnan(vals)]
        if vals.size == 0: return (0.0, 1.0)
        lo, hi = float(vals.min()), float(vals.max())
        if lo == hi: lo -= .5; hi += .5
        m = 0.05*(hi-lo)
        return lo-m, hi+m

    x_min, x_max = _rng([f'x_right_hand_{k}' for k in range(21)] + [f'x_left_hand_{k}' for k in range(21)])
    y_min, y_max = _rng([f'y_right_hand_{k}' for k in range(21)] + [f'y_left_hand_{k}' for k in range(21)])
    z_min, z_max = _rng([f'z_right_hand_{k}' for k in range(21)] + [f'z_left_hand_{k}' for k in range(21)])

    # Helper para trazar l√≠neas de conexiones (segmentos con None)
    def _skeleton_lines(x, y, z):
        Xl, Yl, Zl = [], [], []
        for a,b in HAND_CONNECTIONS:
            Xl += [x[a], x[b], None]
            Yl += [y[a], y[b], None]
            Zl += [z[a], z[b], None]
        return Xl, Yl, Zl

    # Datos iniciales
    Rx,Ry,Rz,Lx,Ly,Lz = _frame_vals(idxs[0])
    rXl,rYl,rZl = _skeleton_lines(Rx,Ry,Rz)
    lXl,lYl,lZl = _skeleton_lines(Lx,Ly,Lz)

    fig = go.Figure(
        data=[
            # Marcadores
            go.Scatter3d(x=Rx, y=Ry, z=Rz, mode='markers', name='Right (pts)',
                         marker=dict(size=4, color=COLORS['primary'])),
            go.Scatter3d(x=Lx, y=Ly, z=Lz, mode='markers', name='Left (pts)',
                         marker=dict(size=4, color=COLORS['accent'])),
            # Esqueleto (l√≠neas)
            go.Scatter3d(x=rXl, y=rYl, z=rZl, mode='lines', name='Right (skel)',
                         line=dict(color=COLORS['primary'], width=3)),
            go.Scatter3d(x=lXl, y=lYl, z=lZl, mode='lines', name='Left (skel)',
                         line=dict(color=COLORS['accent'], width=3)),
        ],
        layout=go.Layout(
            title=f'Hands ‚Äî Frame 1/{len(idxs)}',
            scene=dict(
                xaxis=dict(range=[x_min, x_max], title='X'),
                yaxis=dict(range=[y_min, y_max], title='Y'),
                zaxis=dict(range=[z_min, z_max], title='Z'),
                bgcolor=COLORS['background']
            ),
            paper_bgcolor=COLORS['card'],
            font=dict(color=COLORS['text']),
            height=650,
            updatemenus=[dict(
                type="buttons", showactive=False, y=1.08, x=0.0, xanchor="left",
                buttons=[
                    dict(label="‚ñ∂ Play", method="animate",
                         args=[None, {"frame":{"duration":50,"redraw":True},"fromcurrent":True,"transition":{"duration":0}}]),
                    dict(label="‚è∏ Pause", method="animate",
                         args=[[None], {"frame":{"duration":0,"redraw":False},"mode":"immediate","transition":{"duration":0}}])
                ]
            )],
            sliders=[{
                "active": 0, "y": -0.05, "x": 0.1, "len": 0.8,
                "currentvalue": {"prefix": "Frame: "},
                "steps": [{"args": [[str(k)], {"frame":{"duration":0,"redraw":True},"mode":"immediate"}],
                           "label": str(i+1), "method": "animate"} for i,k in enumerate(idxs)]
            }]
        ),
        frames=[
            (lambda k: go.Frame(
                name=str(k),
                data=[
                    # marcadores R, L
                    go.Scatter3d(x=_frame_vals(k)[0], y=_frame_vals(k)[1], z=_frame_vals(k)[2],
                                 mode='markers', marker=dict(size=4, color=COLORS['primary'])),
                    go.Scatter3d(x=_frame_vals(k)[3], y=_frame_vals(k)[4], z=_frame_vals(k)[5],
                                 mode='markers', marker=dict(size=4, color=COLORS['accent'])),
                    # esqueleto R, L
                    (lambda RX,RY,RZ: go.Scatter3d(
                        x=_skeleton_lines(RX,RY,RZ)[0], y=_skeleton_lines(RX,RY,RZ)[1], z=_skeleton_lines(RX,RY,RZ)[2],
                        mode='lines', line=dict(color=COLORS['primary'], width=3)
                    ))(*_frame_vals(k)[:3]),
                    (lambda LX,LY,LZ: go.Scatter3d(
                        x=_skeleton_lines(LX,LY,LZ)[0], y=_skeleton_lines(LX,LY,LZ)[1], z=_skeleton_lines(LX,LY,LZ)[2],
                        mode='lines', line=dict(color=COLORS['accent'], width=3)
                    ))(*_frame_vals(k)[3:6]),
                ],
                layout=go.Layout(title=f'Hands ‚Äî Frame {idxs.index(k)+1}/{len(idxs)}')
            ))(k)
            for k in idxs
        ]
    )
    return fig

def compute_right_hand_velocity(df):
    # Extrae mano derecha completa
    Rx = df[[f'x_right_hand_{i}' for i in range(21)]].to_numpy(dtype="float32")
    Ry = df[[f'y_right_hand_{i}' for i in range(21)]].to_numpy(dtype="float32")
    Rz = df[[f'z_right_hand_{i}' for i in range(21)]].to_numpy(dtype="float32")
    Rx, Ry, Rz = np.nan_to_num(Rx, nan=0.0), np.nan_to_num(Ry, nan=0.0), np.nan_to_num(Rz, nan=0.0)
    if len(Rx) < 2: 
        return np.array([], dtype="float32")
    dRx, dRy, dRz = np.diff(Rx, axis=0), np.diff(Ry, axis=0), np.diff(Rz, axis=0)
    vel = np.sqrt(np.sum(dRx**2 + dRy**2 + dRz**2, axis=1))  # (t-1,)
    return vel

def build_corr_df(df, hands=('Right',), axes=('X','Y'), landmarks=range(21), max_features=60):
    sel_cols = []
    axis_map = {'X':'x', 'Y':'y', 'Z':'z'}
    hand_map = {'Right':'right_hand', 'Left':'left_hand'}
    for hand in hands:
        for ax in axes:
            prefix = f"{axis_map[ax]}_{hand_map[hand]}_"
            for i in landmarks:
                col = f"{prefix}{i}"
                if col in df.columns:
                    sel_cols.append(col)

    sel_cols = list(dict.fromkeys(sel_cols))  # unique, keep order
    if len(sel_cols) == 0:
        return pd.DataFrame()

    # Limitar features para heatmap legible
    if len(sel_cols) > max_features:
        sel_cols = sel_cols[:max_features]

    sub = df[sel_cols].copy()
    sub = sub.apply(pd.to_numeric, errors='coerce')
    sub = sub.fillna(method='ffill').fillna(method='bfill').fillna(0.0)
    return sub

def build_feature_df(df, hands=('Right',), axes=('X','Y'), landmarks=range(21), max_features=40):
    """Construye un DataFrame con columnas de mano/eje/landmark seleccionados."""
    sel_cols = []
    axis_map = {'X':'x', 'Y':'y', 'Z':'z'}
    hand_map = {'Right':'right_hand', 'Left':'left_hand'}
    for hand in hands:
        for ax in axes:
            prefix = f"{axis_map[ax]}_{hand_map[hand]}_"
            for i in landmarks:
                col = f"{prefix}{i}"
                if col in df.columns:
                    sel_cols.append(col)
    sel_cols = list(dict.fromkeys(sel_cols))
    if len(sel_cols) == 0:
        return pd.DataFrame()

    # L√≠mite para mantener gr√°ficos √°giles
    if len(sel_cols) > max_features:
        sel_cols = sel_cols[:max_features]

    sub = df[sel_cols].copy()
    # limpieza suave
    sub = sub.apply(pd.to_numeric, errors='coerce')
    sub = sub.fillna(method='ffill').fillna(method='bfill').fillna(0.0)
    return sub

def pca_2d_numpy(dataframe):
    """PCA 2D sin dependencias externas (centrado + SVD). Devuelve coords y varianza explicada."""
    X = dataframe.to_numpy(dtype="float64")
    X = np.nan_to_num(X, nan=0.0)
    X = X - X.mean(axis=0, keepdims=True)
    # SVD
    U, S, VT = np.linalg.svd(X, full_matrices=False)
    comps = VT[:2]                 # (2, F)
    coords = X @ comps.T           # (N, 2)
    var_total = (S**2).sum()
    var_exp = (S[:2]**2) / var_total if var_total > 0 else np.array([0.0, 0.0])
    return coords, var_exp

def compact_labels(cols):
    """Etiquetas compactas R/L + eje (X/Y/Z) para los gr√°ficos."""
    labels = []
    for c in cols:
        lbl = c.replace('right_hand','R').replace('left_hand','L')
        lbl = lbl.replace('x_','X.').replace('y_','Y.').replace('z_','Z.')
        labels.append(lbl)
    return labels

# Funci√≥n principal
def main():
    # Header
    st.markdown("""
    <h1>ü§ü ASL Fingerspelling Recognition</h1>
    <p style='text-align: center; color: #7F8C8D; font-size: 1.2em;'>
        Sistema de reconocimiento de deletreo en lenguaje de se√±as americano
    </p>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, {COLORS['primary']} 0%, {COLORS['secondary']} 100%); 
                    padding: 20px; border-radius: 10px; color: white; margin-bottom: 20px;'>
            <h2 style='color: white; margin: 0;'>‚öôÔ∏è Configuraci√≥n</h2>
        </div>
        """, unsafe_allow_html=True)

        # Navegaci√≥n
        st.subheader("üìë Navegaci√≥n")
        page = st.radio(
            "Selecciona una p√°gina:",
            ["üè† Inicio", "üìä Exploraci√≥n de Datos", "üéØ Predicci√≥n", "üìà M√©tricas del Modelo"],
            label_visibility="collapsed"
        )

        # Carga autom√°tica de modelos (1 o 2)
        st.subheader("üì¶ Modelos (autom√°tico)")
        models = {}
        for nice_name, mpath in MODEL_PATHS.items():
            if os.path.exists(mpath):
                m = load_model(mpath)
                if m:
                    models[nice_name] = m
                    st.success(f"‚úÖ {nice_name} cargado: {mpath}")
                else:
                    st.error(f"‚ùå No se pudo cargar {nice_name} ({mpath}).")
            else:
                st.warning(f"‚ö†Ô∏è No se encontr√≥ {mpath} para {nice_name}.")

        # Mapeo de caracteres (se mantiene)
        st.subheader("üî§ Mapeo de Caracteres (autom√°tico)")
        char_to_num, num_to_char = None, None
        if os.path.exists(CHAR_MAP_PATH):
            char_to_num, num_to_char = load_char_map(CHAR_MAP_PATH)
            if char_to_num:
                st.success(f"‚úÖ Mapeo cargado: {CHAR_MAP_PATH}")
            else:
                st.error("‚ùå Error al cargar el mapeo.")
        else:
            st.error(f"‚ùå No se encontr√≥ {CHAR_MAP_PATH}.")

        # Selector de modelos a usar (uno o ambos)
        st.subheader("üß† Selecci√≥n de modelo(s)")
        if models:
            selected_models = st.multiselect(
                "Elige el/los modelo(s):",
                options=list(models.keys()),
                default=[list(models.keys())[0]],
                help="Puedes elegir uno o ambos para comparar."
            )
        else:
            selected_models = []

        # Configs
        st.subheader("üé® Configuraciones")
        show_metrics = st.checkbox("Mostrar m√©tricas", value=True)
        temperature = st.slider("Temperatura de predicci√≥n", 0.1, 2.0, 0.8, 0.1)

        st.markdown("---")
        st.markdown("""
        <div style='text-align: center; color: #95A5A6; font-size: 0.9em;'>
            <p>Desarrollado con ‚ù§Ô∏è</p>
            <p>Streamlit + TensorFlow</p>
        </div>
        """, unsafe_allow_html=True)

        

    # P√°ginas
    if page == "üè† Inicio":
        st.markdown("""
        <div class='info-box'>
            <h2>üëã Bienvenido al Sistema ASL Fingerspelling</h2>
            <p style='font-size: 1.1em;'>
                Esta aplicaci√≥n permite analizar datos de movimientos de manos y realizar 
                predicciones de deletreo en lenguaje de se√±as americano (ASL) utilizando 
                deep learning.
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown(f"""
            <div style='background-color: {COLORS['card']}; padding: 20px; border-radius: 10px; 
                        border-left: 4px solid {COLORS['primary']}; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                <h3 style='color: {COLORS['primary']};'>üìä Exploraci√≥n</h3>
                <p>Visualiza y analiza las caracter√≠sticas de tus datos con gr√°ficas interactivas.</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div style='background-color: {COLORS['card']}; padding: 20px; border-radius: 10px; 
                        border-left: 4px solid {COLORS['accent']}; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                <h3 style='color: {COLORS['accent']};'>üéØ Predicci√≥n</h3>
                <p>Realiza predicciones en nuevos datos usando el modelo entrenado.</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div style='background-color: {COLORS['card']}; padding: 20px; border-radius: 10px; 
                        border-left: 4px solid {COLORS['success']}; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                <h3 style='color: {COLORS['success']};'>üìà M√©tricas</h3>
                <p>Eval√∫a el rendimiento del modelo con m√©tricas detalladas.</p>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("### üöÄ Para comenzar:")
        st.markdown(f"""
        1. Aseg√∫rate de que **{MODEL_PATH}** y **{CHAR_MAP_PATH}** est√©n en la misma carpeta que esta app.
        2. Ve a **üìä Exploraci√≥n de Datos** para analizar un Parquet.
        3. En **üéØ Predicci√≥n**, carga un Parquet y obt√©n el texto predicho.
        """)
        
        st.markdown("### üìã Caracter√≠sticas del Sistema:")
        
        features_col1, features_col2 = st.columns(2)
        
        with features_col1:
            st.markdown(f"""
            - ‚ú® **Visualizaci√≥n 3D** de landmarks
            - üìä **An√°lisis temporal** de movimientos
            - üîÑ **Gr√°ficas interactivas** y enlazadas
            - üé® **Dashboard intuitivo** con teor√≠a del color
            """)
        
        with features_col2:
            st.markdown(f"""
            - ü§ñ **Predicci√≥n en tiempo real**
            - üìà **M√©tricas de rendimiento** detalladas
            - üíæ **Carga de datos** en formato Parquet
            - üéõÔ∏è **Configuraci√≥n ajustable** de par√°metros
            """)
    
    elif page == "üìä Exploraci√≥n de Datos":
        st.markdown("<h2>üìä Exploraci√≥n Interactiva de Datos</h2>", unsafe_allow_html=True)
        
        st.markdown("""
        <div class='info-box'>
            <p>Carga un archivo Parquet para explorar las caracter√≠sticas y variables del conjunto de datos.</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Cargar archivo
        uploaded_file = st.file_uploader("üìÅ Cargar archivo Parquet", type=['parquet'])
        
        if uploaded_file:
            # Cargar datos
            df = load_parquet_data(uploaded_file)
            
            if df is not None:
                st.markdown("<div class='success-box'>‚úÖ Archivo cargado exitosamente</div>", unsafe_allow_html=True)
                
                # Informaci√≥n general
                st.markdown("### üìã Informaci√≥n General")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown(f"""
                    <div class='metric-card'>
                        <div class='metric-label'>Frames</div>
                        <div class='metric-value'>{len(df)}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"""
                    <div class='metric-card'>
                        <div class='metric-label'>Variables</div>
                        <div class='metric-value'>{len(df.columns)}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col3:
                    missing = df.isnull().sum().sum()
                    st.markdown(f"""
                    <div class='metric-card'>
                        <div class='metric-label'>Valores Faltantes</div>
                        <div class='metric-value'>{missing}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col4:
                    memory = df.memory_usage(deep=True).sum() / 1024**2
                    st.markdown(f"""
                    <div class='metric-card'>
                        <div class='metric-label'>Memoria (MB)</div>
                        <div class='metric-value'>{memory:.2f}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                st.markdown("---")
                
                # Tabs para diferentes visualizaciones
                tab1, tab2, tab3, tab4 = st.tabs(["üé® Visualizaci√≥n 3D", "‚è±Ô∏è An√°lisis Temporal", "üìä Distribuciones", "üî• Correlaciones"])
                
                with tab1:
                    st.markdown("### üé® Visualizaci√≥n 3D de Landmarks")
                    st.markdown("""
                    <div class='info-box'>
                        <p>Animaci√≥n interactiva de toda la secuencia. Usa Play/Pause o desliza para explorar frames.</p>
                    </div>
                    """, unsafe_allow_html=True)

                    # Puedes exponer estos controles al usuario si quieres:
                    step = 2          # submuestreo (1 = todos los frames)
                    max_frames = 300  # l√≠mite superior

                    with st.spinner("Preparando animaci√≥n‚Ä¶"):
                        anim_fig = create_landmark_animation_3d(df, step=step, max_frames=max_frames)
                        st.plotly_chart(anim_fig, use_container_width=True)
                                
                with tab2:
                    st.markdown("### ‚è±Ô∏è An√°lisis Temporal de Movimientos")
                    st.markdown("""
                    <div class='info-box'>
                        <p>An√°lisis de la velocidad de movimiento a lo largo del tiempo.</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    with st.spinner("Calculando velocidades..."):
                        fig_temporal = create_temporal_analysis(df)
                        st.plotly_chart(fig_temporal, use_container_width=True)
                    
                    # Estad√≠sticas adicionales
                    st.markdown("#### üìä Estad√≠sticas de Movimiento")
                    vel = compute_right_hand_velocity(df)
                    if vel.size == 0:
                        v_mean = v_max = v_std = 0.0
                    else:
                        v_mean = float(np.nanmean(vel))
                        v_max  = float(np.nanmax(vel))
                        v_std  = float(np.nanstd(vel))

                    stat_col1, stat_col2, stat_col3 = st.columns(3)
                    with stat_col1: st.metric("Velocidad Media", f"{v_mean:.4f}")
                    with stat_col2: st.metric("Velocidad M√°xima", f"{v_max:.4f}")
                    with stat_col3: st.metric("Desv. Est√°ndar", f"{v_std:.4f}")
                
                with tab3:
                    st.markdown("### üìä Distribuci√≥n de Coordenadas")
                    st.markdown("""
                    <div class='info-box'>
                        <p>Distribuci√≥n de las coordenadas X de todos los landmarks.</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    with st.spinner("Generando histograma..."):
                        fig_dist = create_coordinate_distribution(df)
                        st.plotly_chart(fig_dist, use_container_width=True)
                    
                    # Selector para otras coordenadas
                    coord_type = st.selectbox("Selecciona tipo de coordenada:", ["X", "Y", "Z"])
                    
                    if coord_type == "Y":
                        all_coords = df[Y].values.flatten()
                    elif coord_type == "Z":
                        all_coords = df[Z].values.flatten()
                    else:
                        all_coords = df[X].values.flatten()
                    
                    all_coords = all_coords[~np.isnan(all_coords)]
                    
                    fig = go.Figure()
                    fig.add_trace(go.Histogram(
                        x=all_coords,
                        nbinsx=50,
                        name=f'Coordenadas {coord_type}',
                        marker_color=COLORS['accent']
                    ))
                    fig.update_layout(
                        title=f'Distribuci√≥n de Coordenadas {coord_type}',
                        xaxis_title=f'Valor de {coord_type}',
                        yaxis_title='Frecuencia',
                        paper_bgcolor=COLORS['card'],
                        plot_bgcolor=COLORS['background'],
                        font=dict(color=COLORS['text']),
                        height=400
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with tab4:
                    st.markdown("### üß≠ Explorador de Relaciones entre Features (manos)")
                    st.markdown("""
                    <div class='info-box'>
                        <p>Elige mano(s), ejes y landmarks para visualizar relaciones: Matriz de dispersi√≥n, Correlaciones o PCA 2D.</p>
                    </div>
                    """, unsafe_allow_html=True)

                    top_c1, top_c2 = st.columns([1.4, 1.0])
                    with top_c1:
                        vis_type = st.selectbox("Tipo de visualizaci√≥n",
                                                ["Matriz de dispersi√≥n (SPLOM)", "Mapa de calor de correlaci√≥n", "PCA 2D"],
                                                index=0)
                    with top_c2:
                        sample_rows = st.slider("Muestreo de filas (para agilizar)", 200, 5000, 1500, 100)

                    c1, c2, c3 = st.columns([1.2,1.2,1.6])
                    with c1:
                        hands_sel = st.multiselect("Manos", options=["Right","Left"], default=["Right","Left"])
                    with c2:
                        axes_sel = st.multiselect("Ejes", options=["X","Y","Z"], default=["X","Y"])
                    with c3:
                        idxs = st.multiselect("Landmarks (0‚Äì20)", options=list(range(21)), default=list(range(21)))

                    feat_df = build_feature_df(df, hands=tuple(hands_sel), axes=tuple(axes_sel), landmarks=idxs, max_features=40)

                    if feat_df.empty or feat_df.shape[1] < 2:
                        st.warning("Selecciona al menos 2 features v√°lidas para visualizar.")
                    else:
                        # Muestreo de filas para rendimiento
                        if len(feat_df) > sample_rows:
                            feat_df = feat_df.sample(sample_rows, random_state=42).reset_index(drop=True)

                        # Etiquetas compactas
                        feat_df.columns = compact_labels(feat_df.columns)

                        if vis_type == "Matriz de dispersi√≥n (SPLOM)":
                            import plotly.express as px
                            # Limitar columnas a 8‚Äì10 para no saturar
                            max_cols = 9
                            if feat_df.shape[1] > max_cols:
                                st.info(f"Mostrando {max_cols} de {feat_df.shape[1]} features para mantener la gr√°fica √°gil.")
                                feat_df_plot = feat_df.iloc[:, :max_cols]
                            else:
                                feat_df_plot = feat_df

                            fig = px.scatter_matrix(
                                feat_df_plot,
                                dimensions=list(feat_df_plot.columns),
                                opacity=0.6,
                                height=700
                            )
                            fig.update_layout(
                                paper_bgcolor=COLORS['card'],
                                plot_bgcolor=COLORS['background'],
                                font=dict(color=COLORS['text'])
                            )
                            st.plotly_chart(fig, use_container_width=True)

                        elif vis_type == "Mapa de calor de correlaci√≥n":
                            corr = feat_df.corr()
                            fig = go.Figure(data=go.Heatmap(
                                z=corr.values,
                                x=list(corr.columns),
                                y=list(corr.columns),
                                colorscale='RdBu',
                                zmid=0
                            ))
                            fig.update_layout(
                                title=f'Correlaci√≥n ({corr.shape[1]} features)',
                                paper_bgcolor=COLORS['card'],
                                font=dict(color=COLORS['text']),
                                height=600
                            )
                            st.plotly_chart(fig, use_container_width=True)

                        else:  # "PCA 2D"
                            import plotly.express as px
                            coords, var_exp = pca_2d_numpy(feat_df)
                            pc_df = pd.DataFrame({'PC1': coords[:,0], 'PC2': coords[:,1]})
                            caption = f"Varianza explicada: PC1={var_exp[0]*100:.1f}%  ‚Ä¢  PC2={var_exp[1]*100:.1f}%"
                            fig = px.scatter(pc_df, x="PC1", y="PC2", opacity=0.7, height=600)
                            fig.update_layout(
                                title=caption,
                                paper_bgcolor=COLORS['card'],
                                plot_bgcolor=COLORS['background'],
                                font=dict(color=COLORS['text'])
                            )
                            st.plotly_chart(fig, use_container_width=True)

                        with st.expander("üîé Variables incluidas"):
                            st.write(list(feat_df.columns))

                # Vista de datos
                st.markdown("---")
                st.markdown("### üìÑ Vista de Datos")
                
                show_data = st.checkbox("Mostrar datos crudos")
                if show_data:
                    st.dataframe(df.head(100), use_container_width=True, height=400)
                    
                    # Opci√≥n de descarga
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "üì• Descargar datos como CSV",
                        csv,
                        "landmarks_data.csv",
                        "text/csv",
                        key='download-csv'
                    )
        else:
            st.markdown("""
            <div class='warning-box'>
                ‚ö†Ô∏è Por favor, carga un archivo Parquet para comenzar la exploraci√≥n.
            </div>
            """, unsafe_allow_html=True)
    
    elif page == "üéØ Predicci√≥n":
        st.markdown("<h2>üéØ Sistema de Predicci√≥n</h2>", unsafe_allow_html=True)

        if not selected_models or not char_to_num:
            st.markdown(f"""
            <div class='warning-box'>
                ‚ö†Ô∏è Selecciona al menos un modelo en la barra lateral y aseg√∫rate de tener el mapeo de caracteres: <code>{CHAR_MAP_PATH}</code>.
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class='success-box'>
                ‚úÖ Sistema listo para realizar predicciones con el/los modelo(s) seleccionado(s)
            </div>
            """, unsafe_allow_html=True)

            pred_file = st.file_uploader("üìÅ Cargar archivo Parquet para predicci√≥n", type=['parquet'], key='pred_file')
            if pred_file:
                df = load_parquet_data(pred_file)
                if df is not None:
                    st.markdown(f"""
                    <div class='info-box'>
                        üìä Datos cargados: {len(df)} frames con {len(df.columns)} variables
                    </div>
                    """, unsafe_allow_html=True)

                    col_btn = st.container()
                    with col_btn:
                        col1, col2, col3 = st.columns([1, 2, 1])
                        with col2:
                            if st.button("üöÄ Realizar Predicci√≥n", use_container_width=True):
                                results = []
                                for nice_name in selected_models:
                                    m = models[nice_name]
                                    try:
                                        t0 = time.time()
                                        pred_text = predict_sequence(m, df, char_to_num, num_to_char, temperature)
                                        elapsed = time.time() - t0
                                    except Exception as e:
                                        pred_text = None
                                        elapsed = None
                                        st.error(f"‚ùå Error con {nice_name}: {e}")

                                    results.append({
                                        "modelo": nice_name,
                                        "prediccion": pred_text if pred_text is not None else "(error)",
                                        "tiempo_s": elapsed if elapsed is not None else float("nan"),
                                        "frames": len(df),
                                        "longitud": len(pred_text) if pred_text else 0
                                    })

                                # Mostrar resultados por modelo
                                st.markdown("---")
                                st.markdown("### üéâ Resultados por Modelo")
                                cols = st.columns(len(results)) if len(results) > 0 else []
                                for i, r in enumerate(results):
                                    with cols[i]:
                                        st.markdown(f"""
                                        <div style='background: linear-gradient(135deg, {COLORS['success']} 0%, {COLORS['primary']} 100%);
                                                    padding: 20px; border-radius: 12px; text-align: center; color: white;
                                                    box-shadow: 0 6px 12px rgba(0,0,0,0.15); margin: 10px 0;'>
                                            <h3 style='margin: 0 0 8px 0; color: white;'>{r['modelo']}</h3>
                                            <div style='font-size: 1.1em; opacity: 0.9;'>Texto predicho</div>
                                            <div style='font-size: 1.8em; font-weight: 700; margin: 6px 0 0 0;'>" {r['prediccion']} "</div>
                                        </div>
                                        """, unsafe_allow_html=True)
                                        m1, m2 = st.columns(2)
                                        with m1: st.metric("‚è±Ô∏è Tiempo (s)", f"{r['tiempo_s']:.2f}" if not np.isnan(r['tiempo_s']) else "‚Äî")
                                        with m2: st.metric("üî§ Longitud", f"{r['longitud']}")

                                # Si hay 2 modelos: tabla comparativa r√°pida
                                if len(results) >= 2:
                                    st.markdown("---")
                                    st.markdown("### ‚öñÔ∏è Comparaci√≥n r√°pida")
                                    comp_df = pd.DataFrame(results)[["modelo", "tiempo_s", "longitud", "frames"]]
                                    comp_df = comp_df.rename(columns={"modelo":"Modelo", "tiempo_s":"Tiempo (s)", "longitud":"Longitud pred.", "frames":"Frames"})
                                    st.dataframe(comp_df, use_container_width=True, height=160)

                                # Visualizaci√≥n de los datos de entrada (animaci√≥n + temporal)
                                st.markdown("---")
                                st.markdown("### üìä Visualizaci√≥n de los Datos de Entrada")
                                viz_tab1, viz_tab2 = st.tabs(["üé¨ Animaci√≥n 3D (manos)", "‚è±Ô∏è An√°lisis Temporal"])
                                with viz_tab1:
                                    step = 2
                                    max_frames = 300
                                    with st.spinner("Preparando animaci√≥n 3D‚Ä¶"):
                                        anim_fig = create_landmark_animation_3d(df, step=step, max_frames=max_frames)
                                        st.plotly_chart(anim_fig, use_container_width=True)
                                with viz_tab2:
                                    fig_temporal = create_temporal_analysis(df)
                                    st.plotly_chart(fig_temporal, use_container_width=True)
            else:
                st.markdown("""
                <div class='info-box'>
                    üëÜ Carga un archivo Parquet para realizar una predicci√≥n
                </div>
                """, unsafe_allow_html=True)
            
    elif page == "üìà M√©tricas del Modelo":
        st.markdown("<h2>üìà M√©tricas y Rendimiento del Modelo</h2>", unsafe_allow_html=True)

        if not show_metrics:
            st.markdown("""
            <div class='info-box'>
                ‚ÑπÔ∏è Las m√©tricas est√°n ocultas. Activa "Mostrar m√©tricas" en la barra lateral para verlas.
            </div>
            """, unsafe_allow_html=True)
        else:
            if not selected_models:
                st.markdown("""
                <div class='warning-box'>
                    ‚ö†Ô∏è Selecciona al menos un modelo en la barra lateral.
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown("""
                <div class='success-box'>
                    ‚úÖ Modelo(s) cargado(s) ‚Äî Visualizando informaci√≥n
                </div>
                """, unsafe_allow_html=True)

                # Resumen comparativo de modelos
                summary_rows = []
                for nice_name in selected_models:
                    m = models[nice_name]
                    total_params = m.count_params()
                    num_layers = len(m.layers)
                    summary_rows.append({
                        "Modelo": nice_name,
                        "Capas": num_layers,
                        "Par√°metros": total_params
                    })

                st.markdown("### üßæ Resumen comparativo")
                sum_df = pd.DataFrame(summary_rows)
                st.dataframe(sum_df, use_container_width=True, height=120)

                # Barras comparativas de par√°metros y capas (si hay 2)
                if len(summary_rows) >= 1:
                    fig_b1 = go.Figure()
                    fig_b1.add_trace(go.Bar(
                        x=[r["Modelo"] for r in summary_rows],
                        y=[r["Par√°metros"] for r in summary_rows],
                        text=[f"{r['Par√°metros']:,}" for r in summary_rows],
                        textposition="outside",
                        marker_color=[COLORS['primary'], COLORS['secondary']][:len(summary_rows)]
                    ))
                    fig_b1.update_layout(
                        title="Par√°metros por modelo",
                        yaxis_title="N√∫mero de par√°metros",
                        paper_bgcolor=COLORS['card'],
                        plot_bgcolor=COLORS['background'],
                        font=dict(color=COLORS['text']),
                        height=450
                    )
                    st.plotly_chart(fig_b1, use_container_width=True)

                    fig_b2 = go.Figure()
                    fig_b2.add_trace(go.Bar(
                        x=[r["Modelo"] for r in summary_rows],
                        y=[r["Capas"] for r in summary_rows],
                        text=[str(r["Capas"]) for r in summary_rows],
                        textposition="outside",
                        marker_color=[COLORS['accent'], COLORS['success']][:len(summary_rows)]
                    ))
                    fig_b2.update_layout(
                        title="Capas por modelo",
                        yaxis_title="Capas",
                        paper_bgcolor=COLORS['card'],
                        plot_bgcolor=COLORS['background'],
                        font=dict(color=COLORS['text']),
                        height=400
                    )
                    st.plotly_chart(fig_b2, use_container_width=True)

                # Detalle de capas (para el/los seleccionados)
                st.markdown("---")
                st.markdown("### üîç Detalle de capas")
                for nice_name in selected_models:
                    m = models[nice_name]
                    st.markdown(f"#### {nice_name}")
                    layer_info = []
                    for layer in m.layers:
                        layer_info.append({
                            'Nombre': layer.name,
                            'Tipo': layer.__class__.__name__,
                            'Output Shape': str(getattr(layer, 'output_shape', 'N/A')),
                            'Par√°metros': layer.count_params()
                        })
                    st.dataframe(pd.DataFrame(layer_info), use_container_width=True, height=320)


if __name__ == "__main__":
    main()