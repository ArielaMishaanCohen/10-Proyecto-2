{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":52950,"databundleVersionId":5973250,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":8.670361,"end_time":"2025-08-31T22:49:53.385239","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-31T22:49:44.714878","version":"2.6.0"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Proyecto 2\nAriela Mishaan (22052), Alina Car√≠as (22539), Diego Soto (22737), Ignacio M√©ndez (22613) y Marcos D√≠az","metadata":{"_cell_guid":"cde1f4e5-5435-4f62-b525-a9e0d583a111","_uuid":"1ebb8818-a1d8-4afb-90cf-5e5ff7b1b8cd","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.00198,"end_time":"2025-08-31T22:49:50.538616","exception":false,"start_time":"2025-08-31T22:49:50.536636","status":"completed"},"tags":[],"id":"bfdb2f27"}},{"cell_type":"code","source":"import os, json, math, random, time, csv, datetime, warnings, unicodedata, re as _re, sys\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Tuple\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:55:43.778645Z","iopub.execute_input":"2025-11-09T17:55:43.779259Z","iopub.status.idle":"2025-11-09T17:56:02.063391Z","shell.execute_reply.started":"2025-11-09T17:55:43.779237Z","shell.execute_reply":"2025-11-09T17:56:02.062645Z"}},"outputs":[{"name":"stderr","text":"2025-11-09 17:55:48.844575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762710949.080150      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762710949.153753      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"try:\n    tqdm.get_lock().locks = []\nexcept Exception:\n    pass\n\nplt.style.use('seaborn-v0_8-darkgrid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:22.024977Z","iopub.execute_input":"2025-11-09T17:56:22.025308Z","iopub.status.idle":"2025-11-09T17:56:22.030267Z","shell.execute_reply.started":"2025-11-09T17:56:22.025263Z","shell.execute_reply":"2025-11-09T17:56:22.029347Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==== Reporter utilities (versi√≥n para Notebook/Kaggle) ====\nclass Reporter:\n    \"\"\"Reporter para notebook: no redirige sys.stdout; escribe a archivos y deja ver prints en celda.\"\"\"\n    def __init__(self, run_dir=\"./asl_results\", run_name=None):\n        ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        self.run_dir = os.path.abspath(run_dir)\n        os.makedirs(self.run_dir, exist_ok=True)\n        self.run_name = run_name or f\"run-{ts}\"\n        self.run_path = os.path.join(self.run_dir, self.run_name)\n        os.makedirs(self.run_path, exist_ok=True)\n\n        self.log_path = os.path.join(self.run_path, \"console.log\")\n        self.metrics_path = os.path.join(self.run_path, \"metrics.jsonl\")\n        self.summary_path = os.path.join(self.run_path, \"summary.json\")\n        self.pred_csv_path = os.path.join(self.run_path, \"predictions.csv\")\n\n        self._metrics = {}\n        print(f\"üìÅ Guardando salida en: {self.run_path}\")\n\n    def log(self, msg=\"\"):\n            print(msg)\n            try:\n                with open(self.log_path, \"a\", encoding=\"utf-8\") as f:\n                    f.write(msg + (\"\\n\" if not msg.endswith(\"\\n\") else \"\"))\n            except Exception:\n                pass\n    \n    def add_metrics(self, group, **metrics):\n        if group not in self._metrics:\n            self._metrics[group] = {}\n        self._metrics[group].update(metrics)\n        with open(self.metrics_path, \"a\", encoding=\"utf-8\") as f:\n            rec = {\"group\": group, **metrics, \"time\": datetime.datetime.now().isoformat()}\n            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n    def save_predictions(self, rows, fieldnames=(\"modelo\",\"real\",\"pred\")):\n        header_needed = not os.path.exists(self.pred_csv_path)\n        with open(self.pred_csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n            w = csv.DictWriter(f, fieldnames=fieldnames)\n            if header_needed:\n                w.writeheader()\n            for r in rows:\n                w.writerow({k: r.get(k, \"\") for k in fieldnames})\n\n    def save_summary(self):\n        with open(self.summary_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self._metrics, f, indent=2, ensure_ascii=False)\n\nGLOBAL_REPORTER = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:24.048245Z","iopub.execute_input":"2025-11-09T17:56:24.048857Z","iopub.status.idle":"2025-11-09T17:56:24.059285Z","shell.execute_reply.started":"2025-11-09T17:56:24.048831Z","shell.execute_reply":"2025-11-09T17:56:24.058520Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def _find_file(root, filename):\n    for dirpath, dirnames, filenames in os.walk(root):\n        if filename in filenames:\n            return os.path.join(dirpath, filename)\n    return None\n\ndef autodetect_asl_paths():\n    # Intento Kaggle\n    kaggle_root = \"/kaggle/input\"\n    candidates = []\n    if os.path.exists(kaggle_root):\n        for d in os.listdir(kaggle_root):\n            cand_dir = os.path.join(kaggle_root, d)\n            if not os.path.isdir(cand_dir):\n                continue\n            meta = _find_file(cand_dir, \"supplemental_metadata.csv\")\n            cmap = _find_file(cand_dir, \"character_to_prediction_index.json\")\n            # Buscar carpeta de landmarks (contiene muchos .parquet)\n            landmarks_dir = None\n            for sub in os.listdir(cand_dir):\n                full = os.path.join(cand_dir, sub)\n                if os.path.isdir(full) and \"landmark\" in sub.lower():\n                    # heur√≠stica: directorio con muchos .parquet\n                    parquet_count = 0\n                    for root2, _, files2 in os.walk(full):\n                        parquet_count += sum(1 for f in files2 if f.endswith(\".parquet\"))\n                        if parquet_count >= 10:\n                            break\n                    if parquet_count >= 1:\n                        landmarks_dir = full\n                        break\n            if meta and cmap and landmarks_dir:\n                candidates.append((meta, cmap, landmarks_dir))\n        if candidates:\n            return candidates[0]\n    # Intento local (actualiza si lo tienes en otra ruta)\n    local_root = os.path.expanduser(\"~/Downloads/asl-fingerspelling\")\n    meta = os.path.join(local_root, \"supplemental_metadata.csv\")\n    cmap = os.path.join(local_root, \"character_to_prediction_index.json\")\n    landmarks = os.path.join(local_root, \"supplemental_landmarks\")\n    if os.path.exists(meta) and os.path.exists(cmap) and os.path.exists(landmarks):\n        return (meta, cmap, landmarks)\n    # Fallback: usa variables de entorno si est√°n definidas\n    meta = os.environ.get(\"ASL_META\", meta)\n    cmap = os.environ.get(\"ASL_CMAP\", cmap)\n    landmarks = os.environ.get(\"ASL_LANDMARKS\", landmarks)\n    return (meta, cmap, landmarks)\n\n_detected_meta, _detected_cmap, _detected_landmarks = autodetect_asl_paths()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:26.538218Z","iopub.execute_input":"2025-11-09T17:56:26.538549Z","iopub.status.idle":"2025-11-09T17:56:26.739098Z","shell.execute_reply.started":"2025-11-09T17:56:26.538521Z","shell.execute_reply":"2025-11-09T17:56:26.738508Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==== Configuraci√≥n ====\n@dataclass\nclass CFG:\n    TRAIN_CSV: str = _detected_meta\n    PARQUET_ROOT: str = _detected_landmarks\n    CHAR_MAP_JSON: str = _detected_cmap\n    OUTPUT_DIR: str = './asl_results'\n    RANDOM_STATE: int = 42\n    SAMPLE_SEQUENCES: int = 1000\n    FRAME_LEN: int = 128\n    BATCH_SIZE: int = 8\n    EPOCHS: int = 200      # <- ENTRENAR√Å 500 √âPOCAS POR MODELO\n    LR: float = 0.01\n    DROPOUT: float = 0.0\n    NUM_HID: int = 64\n    NUM_HEAD: int = 2\n    NUM_FEED_FORWARD: int = 128\n    NUM_LAYERS_ENC: int = 2\n    NUM_LAYERS_DEC: int = 1\n    TARGET_MAXLEN: int = 64\n\nCFG = CFG()\nCFG.TRAIN_CSV    = \"/kaggle/input/asl-fingerspelling/supplemental_metadata.csv\"\nCFG.CHAR_MAP_JSON= \"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\"\nCFG.PARQUET_ROOT = \"/kaggle/input/asl-fingerspelling/supplemental_landmarks\"\nos.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n\nrandom.seed(CFG.RANDOM_STATE)\ntf.random.set_seed(CFG.RANDOM_STATE)\nnp.random.seed(CFG.RANDOM_STATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:29.133978Z","iopub.execute_input":"2025-11-09T17:56:29.134823Z","iopub.status.idle":"2025-11-09T17:56:29.144727Z","shell.execute_reply.started":"2025-11-09T17:56:29.134787Z","shell.execute_reply":"2025-11-09T17:56:29.143937Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"LPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\n\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\nSEL_COLS = X + Y + Z\n\nRHAND_IDX = [i for i, col in enumerate(SEL_COLS) if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(SEL_COLS) if \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(SEL_COLS) if \"pose\" in col and int(col.split('_')[-1]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(SEL_COLS) if \"pose\" in col and int(col.split('_')[-1]) in LPOSE]\n\nPAD_TOKEN = 'P'\nSTART_TOKEN = 'S'\nEND_TOKEN = 'E'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:32.372695Z","iopub.execute_input":"2025-11-09T17:56:32.373016Z","iopub.status.idle":"2025-11-09T17:56:32.381219Z","shell.execute_reply.started":"2025-11-09T17:56:32.372991Z","shell.execute_reply":"2025-11-09T17:56:32.380310Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ==== Helpers ====\n_ALLOWED_CHARS = list(\"abcdefghijklmnopqrstuvwxyz0123456789- '\")\ndef clean_text(s: str) -> str:\n    if s is None:\n        return \"\"\n    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n    s = s.lower()\n    s = _re.sub(r\"[^a-z0-9\\\\- ']\", \"\", s)\n    s = _re.sub(r\"\\\\s+\", \" \", s).strip()\n    return s\n\ndef levenshtein_distance(s1, s2):\n    if len(s1) < len(s2):\n        return levenshtein_distance(s2, s1)\n    if len(s2) == 0:\n        return len(s1)\n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n    return previous_row[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:34.688455Z","iopub.execute_input":"2025-11-09T17:56:34.688743Z","iopub.status.idle":"2025-11-09T17:56:34.695726Z","shell.execute_reply.started":"2025-11-09T17:56:34.688720Z","shell.execute_reply":"2025-11-09T17:56:34.694869Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class MetricsTracker:\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.predictions = []\n        self.ground_truths = []\n        self.losses = []\n        self.times = []\n    def add_batch(self, preds, truths, loss=None, time_taken=None):\n        self.predictions.extend(preds)\n        self.ground_truths.extend(truths)\n        if loss is not None:\n            self.losses.append(loss)\n        if time_taken is not None:\n            self.times.append(time_taken)\n    def calculate_cer(self):\n        total_chars = 0\n        total_errors = 0\n        for pred, truth in zip(self.predictions, self.ground_truths):\n            pred = pred.strip()\n            truth = truth.strip()\n            total_chars += len(truth)\n            total_errors += levenshtein_distance(pred, truth)\n        return (total_errors / total_chars * 100) if total_chars > 0 else 100.0\n    def calculate_wer(self):\n        total_words = 0\n        total_errors = 0\n        for pred, truth in zip(self.predictions, self.ground_truths):\n            pred_words = pred.strip().split()\n            truth_words = truth.strip().split()\n            total_words += len(truth_words)\n            total_errors += levenshtein_distance(' '.join(pred_words), ' '.join(truth_words))\n        return (total_errors / total_words * 100) if total_words > 0 else 100.0\n    def calculate_accuracy(self):\n        correct = sum(1 for p, t in zip(self.predictions, self.ground_truths) if p.strip() == t.strip())\n        return (correct / len(self.predictions) * 100) if self.predictions else 0.0\n    def get_summary(self):\n        return {\n            'cer': self.calculate_cer(),\n            'wer': self.calculate_wer(),\n            'accuracy': self.calculate_accuracy(),\n            'avg_loss': np.mean(self.losses) if self.losses else 0.0,\n            'avg_time': np.mean(self.times) if self.times else 0.0,\n            'total_samples': len(self.predictions)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:37.256766Z","iopub.execute_input":"2025-11-09T17:56:37.257082Z","iopub.status.idle":"2025-11-09T17:56:37.266798Z","shell.execute_reply.started":"2025-11-09T17:56:37.257054Z","shell.execute_reply":"2025-11-09T17:56:37.265998Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ==== Preprocesamiento de landmarks ====\ndef resize_pad(x):\n    if tf.shape(x)[0] < CFG.FRAME_LEN:\n        x = tf.pad(x, ([[0, CFG.FRAME_LEN - tf.shape(x)[0]], [0, 0], [0, 0]]))\n    else:\n        x = tf.image.resize(x, (CFG.FRAME_LEN, tf.shape(x)[1]))\n    return x\n\ndef pre_process(x):\n    if not isinstance(x, tf.Tensor):\n        x = tf.constant(x, dtype=tf.float32)\n\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n\n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n        hand_len = len(LHAND_IDX) // 3\n        hand_x = hand[:, 0*hand_len:1*hand_len]\n        hand_y = hand[:, 1*hand_len:2*hand_len]\n        hand_z = hand[:, 2*hand_len:3*hand_len]\n        hand = tf.concat([1.0 - hand_x, hand_y, hand_z], axis=1)\n        pose_len = len(LPOSE_IDX) // 3\n        pose_x = pose[:, 0*pose_len:1*pose_len]\n        pose_y = pose[:, 1*pose_len:2*pose_len]\n        pose_z = pose[:, 2*pose_len:3*pose_len]\n        pose = tf.concat([1.0 - pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n\n    hand_len = len(LHAND_IDX) // 3\n    hand_x = hand[:, 0*hand_len:1*hand_len]\n    hand_y = hand[:, 1*hand_len:2*hand_len]\n    hand_z = hand[:, 2*hand_len:3*hand_len]\n    hand = tf.stack([hand_x, hand_y, hand_z], axis=-1)\n    mean = tf.math.reduce_mean(hand, axis=1, keepdims=True)\n    std = tf.math.reduce_std(hand, axis=1, keepdims=True)\n    hand = (hand - mean) / (std + 1e-8)\n\n    pose_len = len(LPOSE_IDX) // 3\n    pose_x = pose[:, 0*pose_len:1*pose_len]\n    pose_y = pose[:, 1*pose_len:2*pose_len]\n    pose_z = pose[:, 2*pose_len:3*pose_len]\n    pose = tf.stack([pose_x, pose_y, pose_z], axis=-1)\n\n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (CFG.FRAME_LEN, -1))\n    return x\n\ndef read_char_map(path: str):\n    with open(path, 'r') as f:\n        char_to_num = json.load(f)\n    # agrega tokens especiales\n    nxt = max(char_to_num.values()) + 1\n    char_to_num[PAD_TOKEN] = nxt; nxt += 1\n    char_to_num[START_TOKEN] = nxt; nxt += 1\n    char_to_num[END_TOKEN] = nxt\n    num_to_char = {j: i for i, j in char_to_num.items()}\n    return char_to_num, num_to_char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:40.035951Z","iopub.execute_input":"2025-11-09T17:56:40.036459Z","iopub.status.idle":"2025-11-09T17:56:40.050178Z","shell.execute_reply.started":"2025-11-09T17:56:40.036432Z","shell.execute_reply":"2025-11-09T17:56:40.049342Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cargar mapa de caracteres\nCHAR_TO_NUM, NUM_TO_CHAR = read_char_map(CFG.CHAR_MAP_JSON)\nVOCAB_SIZE = len(CHAR_TO_NUM)\nprint(f\"üìä Vocabulario: {VOCAB_SIZE} tokens\")\n\ndef preprocess_phrase(phrase):\n    phrase = START_TOKEN + phrase + END_TOKEN\n    phrase_tokens = [CHAR_TO_NUM.get(c, 0) for c in phrase]\n    if len(phrase_tokens) < CFG.TARGET_MAXLEN:\n        phrase_tokens = phrase_tokens + [CHAR_TO_NUM[PAD_TOKEN]] * (CFG.TARGET_MAXLEN - len(phrase_tokens))\n    else:\n        phrase_tokens = phrase_tokens[:CFG.TARGET_MAXLEN]\n    return np.array(phrase_tokens, dtype=np.int32)\n\ndef load_parquet_data(file_path: str):\n    try:\n        df = pd.read_parquet(file_path, columns=SEL_COLS)\n        landmarks = df.values.astype(np.float32)\n        processed = pre_process(landmarks)\n        return processed.numpy()\n    except Exception as e:\n        return None\n\ndef cargar_metadata_un_parquet(csv_path: str, sample_n: int = None):\n    print(\"üìÇ Cargando metadata...\")\n    meta = pd.read_csv(csv_path)\n    meta = meta.dropna(subset=['path', 'phrase'])\n    meta = meta[meta['phrase'].str.len() <= 50]\n\n    parquet_counts = meta['path'].value_counts()\n    target_count = sample_n or 100\n    suitable_parquets = parquet_counts[parquet_counts >= target_count]\n    if len(suitable_parquets) == 0:\n        best_parquet = parquet_counts.index[0]\n        meta_filtered = meta[meta['path'] == best_parquet]\n    else:\n        selected_parquet = suitable_parquets.index[0]\n        meta_filtered = meta[meta['path'] == selected_parquet]\n\n    if sample_n is not None and sample_n < len(meta_filtered):\n        meta_filtered = meta_filtered.sample(sample_n, random_state=CFG.RANDOM_STATE)\n\n    def resolve_path(row):\n        p = str(row['path'])\n        fname = os.path.basename(p)\n        full = os.path.join(CFG.PARQUET_ROOT, fname)\n        return full\n\n    meta_filtered = meta_filtered.copy()\n    meta_filtered.loc[:, 'abs_path'] = meta_filtered.apply(resolve_path, axis=1)\n    print(f\"‚úÖ Dataset: {len(meta_filtered)} secuencias\")\n    return meta_filtered.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:43.446062Z","iopub.execute_input":"2025-11-09T17:56:43.446400Z","iopub.status.idle":"2025-11-09T17:56:43.464093Z","shell.execute_reply.started":"2025-11-09T17:56:43.446375Z","shell.execute_reply":"2025-11-09T17:56:43.463347Z"}},"outputs":[{"name":"stdout","text":"üìä Vocabulario: 62 tokens\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"class LandmarkEmbedding(layers.Layer):\n    def __init__(self, num_hid=64):\n        super().__init__()\n        self.dense1 = layers.Dense(num_hid, activation=\"relu\")\n        self.dropout1 = layers.Dropout(CFG.DROPOUT)\n        self.dense2 = layers.Dense(num_hid, activation=\"relu\")\n        self.dropout2 = layers.Dropout(CFG.DROPOUT)\n        self.num_hid = num_hid\n\n    def call(self, x, training=False):\n        if len(x.shape) == 2:\n            x = tf.expand_dims(x, 0)\n        x = self.dense1(x)\n        x = self.dropout1(x, training=training)\n        x = self.dense2(x)\n        x = self.dropout2(x, training=training)\n        seq_len = tf.shape(x)[1]\n        positions = tf.range(start=0, limit=seq_len, delta=1, dtype=tf.float32)\n        positions = positions[:, tf.newaxis]\n        depth = self.num_hid / 2\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = 1 / (10000 ** depths)\n        angle_rads = positions * angle_rates\n        pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)\n        pos_encoding = pos_encoding[tf.newaxis, :, :]\n        return x + pos_encoding\n\nclass TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, num_hid=64):\n        super().__init__()\n        self.emb = layers.Embedding(num_vocab, num_hid)\n        self.num_hid = num_hid\n\n    def call(self, x):\n        x = self.emb(x)\n        seq_len = tf.shape(x)[1]\n        positions = tf.range(start=0, limit=seq_len, delta=1, dtype=tf.float32)\n        positions = positions[:, tf.newaxis]\n        depth = self.num_hid / 2\n        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth\n        angle_rates = 1 / (10000 ** depths)\n        angle_rads = positions * angle_rates\n        pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)\n        pos_encoding = pos_encoding[tf.newaxis, :, :]\n        return x + pos_encoding\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n        self.ffn = keras.Sequential([\n            layers.Dense(feed_forward_dim, activation=\"relu\"),\n            layers.Dropout(rate),\n            layers.Dense(embed_dim)\n        ])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs, training=training)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout_rate)\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout_rate)\n        self.self_dropout = layers.Dropout(dropout_rate)\n        self.enc_dropout = layers.Dropout(dropout_rate)\n        self.ffn_dropout = layers.Dropout(dropout_rate)\n        self.ffn = keras.Sequential([\n            layers.Dense(feed_forward_dim, activation=\"relu\"),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dim)\n        ])\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n\n        target_att = self.self_att(target, target, attention_mask=causal_mask, training=training)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training=training))\n\n        enc_out_att = self.enc_att(target_norm, enc_out, training=training)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out_att, training=training) + target_norm)\n\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training=training))\n        return ffn_out_norm\n\nclass Transformer(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.enc_input = LandmarkEmbedding(num_hid=CFG.NUM_HID)\n        self.dec_input = TokenEmbedding(num_vocab=VOCAB_SIZE, num_hid=CFG.NUM_HID)\n        self.enc_layers = [TransformerEncoder(CFG.NUM_HID, CFG.NUM_HEAD, CFG.NUM_FEED_FORWARD, rate=CFG.DROPOUT) for _ in range(CFG.NUM_LAYERS_ENC)]\n        self.dec_layers = [TransformerDecoder(CFG.NUM_HID, CFG.NUM_HEAD, CFG.NUM_FEED_FORWARD, dropout_rate=CFG.DROPOUT) for _ in range(CFG.NUM_LAYERS_DEC)]\n        self.classifier = layers.Dense(VOCAB_SIZE)\n\n    def call(self, inputs, training=False):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.enc_input(source, training=training)\n        for enc_layer in self.enc_layers:\n            x = enc_layer(x, training=training)\n        enc_out = x\n        y = self.dec_input(target)\n        for dec_layer in self.dec_layers:\n            y = dec_layer(enc_out, y, training=training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        source = batch[0]\n        target = batch[1]\n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input], training=True)\n            one_hot = tf.one_hot(dec_target, depth=VOCAB_SIZE)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, CHAR_TO_NUM[PAD_TOKEN]))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n\n    def generate(self, source, target_start_token_idx, temperature=0.8):\n        bs = tf.shape(source)[0]\n        enc = self.enc_input(source, training=False)\n        for enc_layer in self.enc_layers:\n            enc = enc_layer(enc, training=False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        for i in range(CFG.TARGET_MAXLEN - 1):\n            dec_out = self.dec_input(dec_input)\n            for dec_layer in self.dec_layers:\n                dec_out = dec_layer(enc, dec_out, training=False)\n            logits = self.classifier(dec_out)\n            logits = logits / temperature\n            probabilities = tf.nn.softmax(logits[:, -1, :], axis=-1)\n            next_token = tf.random.categorical(tf.math.log(probabilities + 1e-10), 1, dtype=tf.int32)\n            dec_input = tf.concat([dec_input, next_token], axis=-1)\n            if tf.reduce_all(tf.equal(next_token, CHAR_TO_NUM[END_TOKEN])):\n                break\n        return dec_input\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:46.969158Z","iopub.execute_input":"2025-11-09T17:56:46.969467Z","iopub.status.idle":"2025-11-09T17:56:46.996758Z","shell.execute_reply.started":"2025-11-09T17:56:46.969442Z","shell.execute_reply":"2025-11-09T17:56:46.995847Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# === Augmentaci√≥n suave de landmarks (solo para train) ===\ndef augment_landmarks(arr):\n    # arr: (T, D) float32 normalizado\n    if np.random.rand() < 0.5:\n        arr = arr * np.random.uniform(0.98, 1.02)  # small scale jitter\n    if np.random.rand() < 0.5:\n        arr = arr + np.random.normal(0, 0.01, size=arr.shape)  # gaussian noise\n\n    # temporal jitter (drop o dup 3% de frames)\n    if np.random.rand() < 0.3:\n        T = arr.shape[0]\n        k = max(1, int(0.03 * T))\n        for _ in range(k):\n            if np.random.rand() < 0.5 and T > 2:\n                # drop\n                idx = np.random.randint(0, T)\n                arr = np.delete(arr, idx, axis=0)\n                T -= 1\n            else:\n                # duplicate\n                idx = np.random.randint(0, T)\n                arr = np.insert(arr, idx, arr[idx], axis=0)\n                T += 1\n        # recortar/pad para mantener longitud fija\n        if arr.shape[0] > CFG.FRAME_LEN:\n            arr = arr[:CFG.FRAME_LEN]\n        elif arr.shape[0] < CFG.FRAME_LEN:\n            pad = np.zeros((CFG.FRAME_LEN - arr.shape[0], arr.shape[1]), dtype=arr.dtype)\n            arr = np.concatenate([arr, pad], axis=0)\n\n    return arr.astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:52.312073Z","iopub.execute_input":"2025-11-09T17:56:52.312397Z","iopub.status.idle":"2025-11-09T17:56:52.320157Z","shell.execute_reply.started":"2025-11-09T17:56:52.312370Z","shell.execute_reply":"2025-11-09T17:56:52.319124Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def crear_dataset(meta, batch_size, shuffle, cache_path=None):\n    def data_generator():\n        for idx, row in meta.iterrows():\n            try:\n                landmarks = load_parquet_data(row['abs_path'])\n                if landmarks is not None:\n                    # aplicar augmentaci√≥n solo si shuffle=True (dataset de entrenamiento)\n                    if shuffle:\n                        landmarks = augment_landmarks(landmarks)\n                    phrase_processed = preprocess_phrase(row['phrase'])\n                    yield landmarks, phrase_processed\n            except Exception:\n                continue\n\n    output_signature = (\n        tf.TensorSpec(shape=(CFG.FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)), dtype=tf.float32),\n        tf.TensorSpec(shape=(CFG.TARGET_MAXLEN,), dtype=tf.int32)\n    )\n\n    ds = tf.data.Dataset.from_generator(data_generator, output_signature=output_signature)\n\n    if cache_path is not None:\n        ds = ds.cache(cache_path)  # cache en disco antes de mezclar\n\n    if shuffle:\n        ds = ds.shuffle(1000)\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:56:56.386812Z","iopub.execute_input":"2025-11-09T17:56:56.387089Z","iopub.status.idle":"2025-11-09T17:56:56.393619Z","shell.execute_reply.started":"2025-11-09T17:56:56.387069Z","shell.execute_reply":"2025-11-09T17:56:56.392803Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_model(model, train_ds, val_ds, model_name, epochs=CFG.EPOCHS, patience=15, min_delta=0.002, save_best=True):\n    print(f\"\\n{'='*60}\\nüèãÔ∏è Entrenando modelo: {model_name}\\n{'='*60}\")\n    history = {'train_loss': [], 'val_loss': [], 'epoch_times': [], 'lr': []}\n\n    # Estado para EarlyStopping y mejor checkpoint\n    best_val = float('inf')\n    best_weights = None\n    best_epoch = -1\n    no_improve = 0\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n\n        # Imprimir solo una vez al principio cuando se cargan los datos\n        if epoch == 0:\n            print(\"üì¶ Cargando datos... Esto solo se muestra una vez.\")\n\n        # ======= ENTRENAMIENTO =======\n        train_losses = []\n        for batch_idx, batch in enumerate(train_ds):\n            loss_dict = model.train_step(batch)\n            loss_value = float(loss_dict['loss'].numpy())\n            train_losses.append(loss_value)\n\n        avg_train_loss = float(np.mean(train_losses))\n        history['train_loss'].append(avg_train_loss)\n\n        # ======= VALIDACI√ìN =======\n        val_losses = []\n        for batch in val_ds:\n            source = batch[0]\n            target = batch[1]\n            dec_input  = target[:, :-1]\n            dec_target = target[:,  1:]\n\n            preds = model([source, dec_input], training=False)  # (B, L, V)\n            one_hot = tf.one_hot(dec_target, depth=VOCAB_SIZE)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, CHAR_TO_NUM[PAD_TOKEN]))\n            loss = keras.losses.categorical_crossentropy(one_hot, preds, from_logits=True)\n            loss = tf.reduce_sum(loss * tf.cast(mask, tf.float32)) / tf.reduce_sum(tf.cast(mask, tf.float32))\n            val_losses.append(float(loss.numpy()))\n\n        avg_val_loss = float(np.mean(val_losses))\n        history['val_loss'].append(avg_val_loss)\n\n        # ======= TIEMPO Y LR =======\n        epoch_time = time.time() - epoch_start\n        history['epoch_times'].append(epoch_time)\n\n        current_lr = model.optimizer.learning_rate\n        try:\n            current_lr = float(current_lr(model.optimizer.iterations).numpy()) if hasattr(current_lr, '__call__') else float(current_lr.numpy())\n        except Exception:\n            current_lr = float(CFG.LR)\n        history['lr'].append(current_lr)\n\n        # ======= LOG DE √âPOCA =======\n        print(f\"\\nüìà √âpoca {epoch+1}/{epochs}:\")\n        print(f\"     Train Loss: {avg_train_loss:.4f}\")\n        print(f\"     Val  Loss:  {avg_val_loss:.4f}\")\n        print(f\"     Time:       {epoch_time:.2f}s\")\n        print(f\"     LR:         {current_lr:.2e}\")\n\n        # ======= EARLY STOPPING & BEST CHECKPOINT =======\n        if avg_val_loss < (best_val - min_delta):\n            best_val = avg_val_loss\n            best_epoch = epoch\n            best_weights = model.get_weights()\n            no_improve = 0\n            print(f\"     üü¢ Mejora en val_loss. Nuevo mejor: {best_val:.4f} (√©poca {best_epoch+1})\")\n        else:\n            no_improve += 1\n            print(f\"     ‚ö†Ô∏è  Sin mejora por {no_improve} √©poca(s) (mejor val_loss {best_val:.4f} en la √©poca {best_epoch+1})\")\n\n        if no_improve >= patience:\n            print(f\"üö® EarlyStopping activado: {no_improve} √©pocas sin mejora (paciencia={patience}).\")\n            print(f\"    Mejor √©poca: {best_epoch+1} con val_loss={best_val:.4f}\")\n            break\n\n    # ======= RESTAURAR MEJORES PESOS Y GUARDAR =======\n    if best_weights is not None:\n        model.set_weights(best_weights)\n        print(f\"‚úÖ Restaurados pesos de la mejor √©poca: {best_epoch+1} (val_loss={best_val:.4f})\")\n\n    if save_best:\n        base = f\"/kaggle/working/{model_name.replace(' ', '_')}_best\"\n        try:\n            # Formato .keras recomendado y sin optimizador (no necesitas optimizer para inferencia)\n            model.save(base + \".keras\", include_optimizer=False)\n            print(f\"üíæ Modelo guardado: {base}.keras\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è No se pudo serializar el modelo completo ({e}). Guardo solo pesos.\")\n            model.save_weights(base + \".weights.h5\")\n            print(f\"üíæ Pesos guardados: {base}.weights.h5\")\n\n\n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:22.348742Z","iopub.execute_input":"2025-11-09T17:57:22.349039Z","iopub.status.idle":"2025-11-09T17:57:22.362774Z","shell.execute_reply.started":"2025-11-09T17:57:22.349016Z","shell.execute_reply":"2025-11-09T17:57:22.361660Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# === Helper: Beam Search por lote (itera por muestra) ===\ndef decode_beam_batch(model, source_batch, start_id, end_id, pad_id,\n                      beam_width=5, max_len=CFG.TARGET_MAXLEN, length_penalty=0.9):\n    \"\"\"\n    Devuelve lista de secuencias (np.int32) decodificadas con beam search.\n    - model: Transformer con forward model([source, dec_input], training=False) -> (B, L, V)\n    - source_batch: tensor (B, T, D)\n    \"\"\"\n    import numpy as np\n\n    def beam_search_single(source_b):\n        # Cada haz: (seq[list[int]], logprob[float], finished[bool])\n        beams = [([start_id], 0.0, False)]\n        for _ in range(max_len - 1):\n            candidates = []\n            all_finished = True\n            for seq, logp, finished in beams:\n                if finished:\n                    candidates.append((seq, logp, True))\n                    continue\n                all_finished = False\n                dec_in = tf.constant([seq], dtype=tf.int32)            # (1, L_dec)\n                src_in = tf.expand_dims(source_b, 0)                   # (1, T, D)\n                logits = model([src_in, dec_in], training=False)       # (1, L_dec, V)\n                next_logits = logits[:, -1, :]                         # (1, V)\n                log_probs = tf.nn.log_softmax(next_logits, axis=-1).numpy()[0]  # (V,)\n\n                # top-k por log_prob\n                k = min(beam_width, log_probs.shape[0])\n                topk_idx = np.argpartition(-log_probs, k-1)[:k]\n                # ordenar esos k por prob\n                topk_idx = topk_idx[np.argsort(-log_probs[topk_idx])]\n\n                for idx in topk_idx:\n                    new_seq = seq + [int(idx)]\n                    new_logp = logp + float(log_probs[idx])\n                    candidates.append((new_seq, new_logp, idx == end_id))\n\n            # Seleccionar los mejores 'beam_width' con penalizaci√≥n de longitud\n            def score_fn(seq, logp):\n                L = max(1, len(seq))\n                return logp / (((5.0 + L) / 6.0) ** length_penalty)\n\n            candidates.sort(key=lambda x: score_fn(x[0], x[1]), reverse=True)\n            beams = candidates[:beam_width]\n\n            # Si todos terminaron, cortamos\n            if all(x[2] for x in beams):\n                break\n\n        # Mejor secuencia final\n        best_seq = beams[0][0]\n        # quitar <START> y truncar en <END>\n        out = []\n        for tid in best_seq[1:]:\n            if tid == end_id:\n                break\n            if tid != pad_id:\n                out.append(tid)\n        # pad/trunc a longitud objetivo\n        out = out[:CFG.TARGET_MAXLEN]\n        if len(out) < CFG.TARGET_MAXLEN:\n            out += [pad_id] * (CFG.TARGET_MAXLEN - len(out))\n        return np.array(out, dtype=np.int32)\n\n    # Procesar batch\n    source_np = source_batch  # tensor (B, T, D)\n    B = int(tf.shape(source_np)[0])\n    decoded = [beam_search_single(source_np[i]) for i in range(B)]\n    return np.stack(decoded, axis=0)  # (B, L)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:25.436770Z","iopub.execute_input":"2025-11-09T17:57:25.437565Z","iopub.status.idle":"2025-11-09T17:57:25.448447Z","shell.execute_reply.started":"2025-11-09T17:57:25.437540Z","shell.execute_reply":"2025-11-09T17:57:25.447484Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def evaluate_model(model, val_ds, val_meta, model_name, temperature=0.8):\n    global GLOBAL_REPORTER\n    print(f\"\\n{'='*60}\\nüîç Evaluando modelo: {model_name}\\n{'='*60}\")\n    tracker = MetricsTracker()\n\n    START_ID = CHAR_TO_NUM[START_TOKEN]\n    END_ID   = CHAR_TO_NUM[END_TOKEN]\n    PAD_ID   = CHAR_TO_NUM[PAD_TOKEN]\n\n    # Evaluaci√≥n por lotes usando Beam Search\n    for batch_idx, batch in enumerate(val_ds):\n        batch_start = time.time()\n        source = batch[0]\n        target = batch[1].numpy()\n\n        # --- Beam Search en vez de greedy/temperature ---\n        preds = decode_beam_batch(\n            model, source,\n            start_id=START_ID, end_id=END_ID, pad_id=PAD_ID,\n            beam_width=5, max_len=CFG.TARGET_MAXLEN, length_penalty=0.9\n        )\n        # preds: (B, L) np.int32\n        batch_time = time.time() - batch_start\n\n        batch_preds, batch_truths = [], []\n        for i in range(len(target)):\n            # Ground truth\n            gt_tokens = []\n            for idx in target[i, :]:\n                if idx == END_ID:\n                    break\n                if idx not in (PAD_ID, START_ID):\n                    gt_tokens.append(NUM_TO_CHAR.get(int(idx), \"\"))\n            ground_truth = \"\".join(gt_tokens)\n            batch_truths.append(ground_truth)\n\n            # Prediction (decodificar ids -> chars)\n            pred_tokens = []\n            for idx in preds[i, :]:\n                ch = NUM_TO_CHAR.get(int(idx), \"\")\n                if ch == END_TOKEN:\n                    break\n                if ch not in (PAD_TOKEN, START_TOKEN):\n                    pred_tokens.append(ch)\n            prediction = \"\".join(pred_tokens)\n            batch_preds.append(prediction)\n\n        tracker.add_batch(batch_preds, batch_truths, time_taken=batch_time)\n\n        if len(tracker.predictions) > 0:\n            partial_cer = tracker.calculate_cer()\n            partial_acc = tracker.calculate_accuracy()\n            print(f\"Evaluaci√≥n: CER={partial_cer:.2f}% | Accuracy={partial_acc:.2f}%\")\n\n    metrics = tracker.get_summary()\n    print(f\"\\nüìä Resultados de {model_name}:\")\n    print(f\"   CER (Character Error Rate): {metrics['cer']:.2f}%\")\n    print(f\"   WER (Word Error Rate):      {metrics['wer']:.2f}%\")\n    print(f\"   Accuracy (Exact Match):     {metrics['accuracy']:.2f}%\")\n    print(f\"   Avg Loss:                   {metrics['avg_loss']:.4f}\")\n    print(f\"   Avg Time per batch:         {metrics['avg_time']:.3f}s\")\n    print(f\"   Total samples:              {metrics['total_samples']}\")\n\n    print(f\"\\nüìù Ejemplos de predicciones:\")\n    for i in range(min(5, len(tracker.predictions))):\n        print(f\"\\n   Ejemplo {i+1}:\")\n        print(f\"   Real: '{tracker.ground_truths[i]}'\")\n        print(f\"   Pred: '{tracker.predictions[i]}'\")\n\n    # --- Reporter writeback ---\n    if GLOBAL_REPORTER is not None:\n        try:\n            GLOBAL_REPORTER.add_metrics(model_name,\n                                        CER=metrics.get('cer', None),\n                                        WER=metrics.get('wer', None),\n                                        Acc=metrics.get('accuracy', None),\n                                        AvgLoss=metrics.get('avg_loss', None),\n                                        AvgTimePerBatch=metrics.get('avg_time', None),\n                                        TotalSamples=metrics.get('total_samples', None))\n            rows = [{\"modelo\": model_name, \"real\": gt, \"pred\": pr}\n                    for gt, pr in zip(tracker.ground_truths, tracker.predictions)]\n            if rows:\n                GLOBAL_REPORTER.save_predictions(rows)\n        except Exception as _e:\n            print(f\"[Reporter] No se pudo guardar m√©tricas/predicciones: {_e}\")\n\n    return metrics, tracker\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:28.369048Z","iopub.execute_input":"2025-11-09T17:57:28.369389Z","iopub.status.idle":"2025-11-09T17:57:28.382053Z","shell.execute_reply.started":"2025-11-09T17:57:28.369366Z","shell.execute_reply":"2025-11-09T17:57:28.381267Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ====== Visualizaciones ======\ndef plot_training_comparison(histories, model_names, save_path):\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('Comparaci√≥n de Entrenamiento entre Modelos', fontsize=16, fontweight='bold')\n\n    # Train loss\n    ax = axes[0, 0]\n    for history, name in zip(histories, model_names):\n        ax.plot(history['train_loss'], marker='o', label=name, linewidth=2)\n    ax.set_xlabel('√âpoca', fontsize=12)\n    ax.set_ylabel('Training Loss', fontsize=12)\n    ax.set_title('P√©rdida en Entrenamiento', fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Val loss\n    ax = axes[0, 1]\n    for history, name in zip(histories, model_names):\n        ax.plot(history['val_loss'], marker='s', label=name, linewidth=2)\n    ax.set_xlabel('√âpoca', fontsize=12)\n    ax.set_ylabel('Validation Loss', fontsize=12)\n    ax.set_title('P√©rdida en Validaci√≥n', fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # epoch times\n    ax = axes[1, 0]\n    x = np.arange(len(histories[0]['epoch_times']))\n    width = 0.35\n    for i, (history, name) in enumerate(zip(histories, model_names)):\n        ax.bar(x + i*width, history['epoch_times'], width, label=name, alpha=0.7)\n    ax.set_xlabel('√âpoca', fontsize=12)\n    ax.set_ylabel('Tiempo (segundos)', fontsize=12)\n    ax.set_title('Tiempo de Entrenamiento por √âpoca', fontsize=14, fontweight='bold')\n    ax.set_xticks(x + width / 2)\n    ax.set_xticklabels([f'{i+1}' for i in range(len(histories[0]['epoch_times']))])\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n\n    # LR\n    ax = axes[1, 1]\n    for history, name in zip(histories, model_names):\n        ax.plot(history['lr'], marker='^', label=name, linewidth=2)\n    ax.set_xlabel('√âpoca', fontsize=12)\n    ax.set_ylabel('Learning Rate', fontsize=12)\n    ax.set_title('Evoluci√≥n del Learning Rate', fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_yscale('log')\n\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"‚úÖ Gr√°fica guardada: {save_path}\")\n    plt.close()\n\n\ndef plot_metrics_comparison(metrics_list, model_names, save_path):\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    fig.suptitle('Comparaci√≥n de M√©tricas de Evaluaci√≥n', fontsize=16, fontweight='bold')\n    metric_names = ['cer', 'wer', 'accuracy']\n    metric_labels = ['CER (%)', 'WER (%)', 'Accuracy (%)']\n    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n    for idx, (metric, label, color) in enumerate(zip(metric_names, metric_labels, colors)):\n        ax = axes[idx]\n        values = [m[metric] for m in metrics_list]\n        bars = ax.bar(model_names, values, color=color, alpha=0.7, edgecolor='black', linewidth=2)\n        for bar, value in zip(bars, values):\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height, f'{value:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n        ax.set_ylabel(label, fontsize=12)\n        ax.set_title(label, fontsize=14, fontweight='bold')\n        ax.grid(True, alpha=0.3, axis='y')\n        ax.set_ylim(0, max(values) * 1.2 if values else 1.0)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"‚úÖ Gr√°fica guardada: {save_path}\")\n    plt.close()\n\ndef plot_error_analysis(trackers, model_names, save_path):\n    fig = plt.figure(figsize=(16, 10))\n    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n    fig.suptitle('An√°lisis Detallado de Errores', fontsize=16, fontweight='bold')\n\n    for idx, (tracker, name) in enumerate(zip(trackers, model_names)):\n        errors = []\n        for pred, truth in zip(tracker.predictions, tracker.ground_truths):\n            error = levenshtein_distance(pred.strip(), truth.strip())\n            errors.append(error)\n\n        ax = fig.add_subplot(gs[0, idx])\n        ax.hist(errors, bins=20, color=f'C{idx}', alpha=0.7, edgecolor='black')\n        ax.set_xlabel('Distancia de Levenshtein', fontsize=11)\n        ax.set_ylabel('Frecuencia', fontsize=11)\n        ax.set_title(f'{name} - Distribuci√≥n de Errores', fontsize=12, fontweight='bold')\n        ax.grid(True, alpha=0.3, axis='y')\n        if errors:\n            ax.axvline(np.mean(errors), color='red', linestyle='--', linewidth=2, label=f'Media: {np.mean(errors):.2f}')\n            ax.legend()\n\n        ax = fig.add_subplot(gs[1, idx])\n        pred_lens = [len(p.strip()) for p in tracker.predictions]\n        truth_lens = [len(t.strip()) for t in tracker.ground_truths]\n        maxlen = max(truth_lens) if truth_lens else 1\n        ax.scatter(truth_lens, pred_lens, alpha=0.5, s=30, c=f'C{idx}')\n        ax.plot([0, maxlen], [0, maxlen], 'r--', linewidth=2, label='Ideal')\n        ax.set_xlabel('Longitud Real', fontsize=11)\n        ax.set_ylabel('Longitud Predicha', fontsize=11)\n        ax.set_title(f'{name} - Longitud Predicciones', fontsize=12, fontweight='bold')\n        ax.grid(True, alpha=0.3)\n        ax.legend()\n\n    ax = fig.add_subplot(gs[2, :])\n    metrics_to_compare = {\n        'CER': [t.calculate_cer() for t in trackers],\n        'WER': [t.calculate_wer() for t in trackers],\n        'Accuracy': [t.calculate_accuracy() for t in trackers]\n    }\n\n    x = np.arange(len(model_names))\n    width = 0.25\n    for i, (metric_name, values) in enumerate(metrics_to_compare.items()):\n        offset = width * (i - 1)\n        bars = ax.bar(x + offset, values, width, label=metric_name, alpha=0.8)\n        for bar, value in zip(bars, values):\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height, f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    ax.set_xlabel('Modelos', fontsize=12)\n    ax.set_ylabel('Porcentaje (%)', fontsize=12)\n    ax.set_title('Comparaci√≥n de M√©tricas por Modelo', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(model_names)\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"‚úÖ Gr√°fica guardada: {save_path}\")\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:31.393967Z","iopub.execute_input":"2025-11-09T17:57:31.394275Z","iopub.status.idle":"2025-11-09T17:57:31.419136Z","shell.execute_reply.started":"2025-11-09T17:57:31.394251Z","shell.execute_reply":"2025-11-09T17:57:31.418053Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ====== HP Switcher ======\ndef apply_hparams(hp: dict):\n    if \"NUM_HID\" in hp:          CFG.NUM_HID = hp[\"NUM_HID\"]\n    if \"NUM_HEAD\" in hp:         CFG.NUM_HEAD = hp[\"NUM_HEAD\"]\n    if \"NUM_FEED_FORWARD\" in hp: CFG.NUM_FEED_FORWARD = hp[\"NUM_FEED_FORWARD\"]\n    if \"NUM_LAYERS_ENC\" in hp:   CFG.NUM_LAYERS_ENC = hp[\"NUM_LAYERS_ENC\"]\n    if \"NUM_LAYERS_DEC\" in hp:   CFG.NUM_LAYERS_DEC = hp[\"NUM_LAYERS_DEC\"]\n    if \"DROPOUT\" in hp:          CFG.DROPOUT = hp[\"DROPOUT\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:34.668854Z","iopub.execute_input":"2025-11-09T17:57:34.669507Z","iopub.status.idle":"2025-11-09T17:57:34.673878Z","shell.execute_reply.started":"2025-11-09T17:57:34.669479Z","shell.execute_reply":"2025-11-09T17:57:34.673216Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# === Scheduler: Warmup + Cosine decay (serializable) ===\nimport math\nimport tensorflow as tf\nfrom tensorflow import keras\n\nclass WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, lr_max=1e-4, total_steps=100000, warmup_ratio=0.1, lr_min=1e-6):\n        super().__init__()\n        self.lr_max = tf.cast(lr_max, tf.float32)\n        self.lr_min = tf.cast(lr_min, tf.float32)\n        self.total_steps = tf.cast(total_steps, tf.float32)\n        self.warmup_steps = tf.cast(total_steps * warmup_ratio, tf.float32)\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        lr_warm = self.lr_max * (step / tf.maximum(1.0, self.warmup_steps))\n        progress = tf.clip_by_value(\n            (step - self.warmup_steps) / tf.maximum(1.0, (self.total_steps - self.warmup_steps)),\n            0.0, 1.0\n        )\n        lr_cos = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1.0 + tf.cos(math.pi * progress))\n        return tf.where(step < self.warmup_steps, lr_warm, lr_cos)\n\n    def get_config(self):\n        # Devuelve tipos nativos para que Keras pueda serializar\n        return {\n            \"lr_max\": float(self.lr_max.numpy()),\n            \"total_steps\": int(self.total_steps.numpy()),\n            \"warmup_ratio\": float((self.warmup_steps.numpy() / max(1.0, self.total_steps.numpy()))),\n            \"lr_min\": float(self.lr_min.numpy()),\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:42.511322Z","iopub.execute_input":"2025-11-09T17:57:42.511599Z","iopub.status.idle":"2025-11-09T17:57:42.519251Z","shell.execute_reply.started":"2025-11-09T17:57:42.511579Z","shell.execute_reply":"2025-11-09T17:57:42.518455Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def main():\n    global GLOBAL_REPORTER\n    print(\"=\"*80)\n    print(\"üöÄ COMPARACI√ìN DE MODELOS: TRANSFORMER-A vs TRANSFORMER-B\")\n    print(\"   ASL Fingerspelling Recognition\")\n    print(\"=\"*80)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìÇ FASE 1: CARGA Y PREPARACI√ìN DE DATOS\")\n    print(\"=\"*80)\n    meta = cargar_metadata_un_parquet(CFG.TRAIN_CSV, sample_n=CFG.SAMPLE_SEQUENCES)\n    trn_df, val_df = train_test_split(meta, test_size=0.2, random_state=CFG.RANDOM_STATE)\n    print(f\"\\n‚úÖ Split completado: train={len(trn_df)}, val={len(val_df)}\")\n\n    print(\"\\nüîÑ Creando datasets...\")\n    train_ds = crear_dataset(trn_df, CFG.BATCH_SIZE, shuffle=True)\n    train_ds = crear_dataset(trn_df, CFG.BATCH_SIZE, shuffle=True,  cache_path='/kaggle/working/train_cache')\n    # Cachear validaci√≥n UNA VEZ y reusarla tanto en \"Val\" (durante entrenamiento) como en \"Evaluaci√≥n final\"\n    val_ds   = crear_dataset(val_df, CFG.BATCH_SIZE, shuffle=False)\n    val_ds   = val_ds.cache('/kaggle/working/val_cache')  # cache persistente en disco\n    val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ü§ñ FASE 2: ENTRENAMIENTO - TRANSFORMERS (A vs B)\")\n    print(\"=\"*80)\n\n    # --- Dos paquetes de hiperpar√°metros ---\n    HP_A = {\n        \"NAME\": \"Transformer-A\",\n        \"NUM_HID\": 64,\n        \"NUM_HEAD\": 2,\n        \"NUM_FEED_FORWARD\": 128,\n        \"NUM_LAYERS_ENC\": 2,\n        \"NUM_LAYERS_DEC\": 1,\n        \"DROPOUT\": 0.10,\n        \"LR\": 3e-4,\n        \"LABEL_SMOOTH\": 0.10,\n        \"TEMP\": 0.8,\n    }\n\n    HP_B = {\n        \"NAME\": \"Transformer-B\",\n        \"NUM_HID\": 512,\n        \"NUM_HEAD\": 8,\n        \"NUM_FEED_FORWARD\": 512,\n        \"NUM_LAYERS_ENC\": 4,\n        \"NUM_LAYERS_DEC\": 3,\n        \"DROPOUT\": 0.35,\n        \"LR\": 1e-4,\n        \"LABEL_SMOOTH\": 0.1,\n        \"TEMP\": 0.7,\n    }\n\n    # Registrar config\n    if GLOBAL_REPORTER is not None:\n        try:\n            GLOBAL_REPORTER.save_summary()  # inicializa\n        except Exception:\n            pass\n\n    histories, metrics_list, trackers, names = [], [], [], []\n\n# ---------- ENTRENAR MODELO A ----------\n   #\n\n# ---------- ENTRENAR MODELO B ----------\n    apply_hparams(HP_B)\n    transformer_B = Transformer()\n\n    # Estimar pasos totales (m√°s estable que fijar 1000)\n    # Usamos math.ceil para no depender de numpy aqu√≠.\n    steps_per_epoch = max(1, math.ceil(len(trn_df) / CFG.BATCH_SIZE))\n    # Si prefieres fijar un tope, puedes usar 200; aqu√≠ uso las √©pocas planeadas.\n    total_steps = steps_per_epoch * CFG.EPOCHS\n\n    # Scheduler Warmup + Cosine\n    lr_schedule_B = WarmupCosine(\n        lr_max=HP_B[\"LR\"],      # pico de LR\n        total_steps=total_steps,\n        warmup_ratio=0.1,      # 10% de warmup\n        lr_min=1e-6\n    )\n\n    # Optimizer AdamW con weight decay y clipping para estabilizar\n    optimizer_B = tf.keras.optimizers.AdamW(\n        learning_rate=lr_schedule_B,\n        weight_decay=5e-4,\n        clipnorm=1.0,\n        beta_1=0.9, beta_2=0.98\n    )\n\n    transformer_B.compile(\n        optimizer=optimizer_B,\n        loss=keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=HP_B[\"LABEL_SMOOTH\"])\n    )\n\n    hist_B = train_model(transformer_B, train_ds, val_ds, HP_B[\"NAME\"], CFG.EPOCHS)\n    metrics_B, tracker_B = evaluate_model(transformer_B, val_ds, val_df, HP_B[\"NAME\"], temperature=HP_B[\"TEMP\"])\n    histories = [hist_B]        # Solo el historial de B\n    metrics_list = [metrics_B]  # Solo las m√©tricas de B\n    trackers = [tracker_B]      # Solo el tracker de B\n    names = [HP_B[\"NAME\"]]\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìà FASE 3: GENERACI√ìN DE VISUALIZACIONES\")\n    print(\"=\"*80)\n    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n    plot_training_comparison(histories, names, os.path.join(CFG.OUTPUT_DIR, 'training_comparison.png'))\n    plot_metrics_comparison(metrics_list, names, os.path.join(CFG.OUTPUT_DIR, 'metrics_comparison.png'))\n    plot_error_analysis(trackers, names, os.path.join(CFG.OUTPUT_DIR, 'error_analysis.png'))\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÜ RESUMEN FINAL Y CONCLUSIONES\")\n    print(\"=\"*80)\n\n    print(\"\\nüìä COMPARACI√ìN DE M√âTRICAS:\")\n    print(f\"\\n{'M√©trica':<20} {names[0]:<15}\")\n    print(\"-\" * 65)\n    metrics_to_show = [\n        ('CER (%)', 'cer', 'menor'),\n        ('WER (%)', 'wer', 'menor'),\n        ('Accuracy (%)', 'accuracy', 'mayor'),\n        ('Avg Loss', 'avg_loss', 'menor')\n    ]\n    for metric_name, metric_key, comparison in metrics_to_show:\n        b_val = metrics_list[0][metric_key]  # Solo accedemos a las m√©tricas de Modelo B\n        print(f\"{metric_name:<20} {b_val:<15.2f}\")\n\n    print(\"\\nüíæ ARCHIVOS GENERADOS:\")\n    print(f\"   üìÅ Directorio: {CFG.OUTPUT_DIR}\")\n    print(f\"   üìä training_comparison.png\")\n    print(f\"   üìä metrics_comparison.png\")\n    print(f\"   üìä error_analysis.png\")\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ AN√ÅLISIS COMPLETADO\")\n    print(\"=\"*80)\n\n# ====== EJECUCI√ìN DIRECTA (en notebook) ======\n# Crea reporter global (opcional)\nGLOBAL_REPORTER = Reporter(run_dir=CFG.OUTPUT_DIR, run_name=\"run-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:57:56.989990Z","iopub.execute_input":"2025-11-09T17:57:56.990690Z","iopub.status.idle":"2025-11-10T04:42:25.011522Z","shell.execute_reply.started":"2025-11-09T17:57:56.990655Z","shell.execute_reply":"2025-11-10T04:42:25.010694Z"}},"outputs":[{"name":"stdout","text":"üìÅ Guardando salida en: /kaggle/working/asl_results/run-20251109-175757\n================================================================================\nüöÄ COMPARACI√ìN DE MODELOS: TRANSFORMER-A vs TRANSFORMER-B\n   ASL Fingerspelling Recognition\n================================================================================\n\n================================================================================\nüìÇ FASE 1: CARGA Y PREPARACI√ìN DE DATOS\n================================================================================\nüìÇ Cargando metadata...\n‚úÖ Dataset: 1000 secuencias\n\n‚úÖ Split completado: train=800, val=200\n\nüîÑ Creando datasets...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1762711078.247144      38 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1762711078.248106      38 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nü§ñ FASE 2: ENTRENAMIENTO - TRANSFORMERS (A vs B)\n================================================================================\n\n============================================================\nüèãÔ∏è Entrenando modelo: Transformer-B\n============================================================\nüì¶ Cargando datos... Esto solo se muestra una vez.\n\nüìà √âpoca 1/200:\n     Train Loss: 2.3041\n     Val  Loss:  3.7649\n     Time:       717.57s\n     LR:         5.00e-06\n     üü¢ Mejora en val_loss. Nuevo mejor: 3.7649 (√©poca 1)\n\nüìà √âpoca 2/200:\n     Train Loss: 2.0860\n     Val  Loss:  3.1181\n     Time:       212.84s\n     LR:         1.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 3.1181 (√©poca 2)\n\nüìà √âpoca 3/200:\n     Train Loss: 1.9371\n     Val  Loss:  2.9516\n     Time:       215.41s\n     LR:         1.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.9516 (√©poca 3)\n\nüìà √âpoca 4/200:\n     Train Loss: 1.8601\n     Val  Loss:  2.9281\n     Time:       213.45s\n     LR:         2.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.9281 (√©poca 4)\n\nüìà √âpoca 5/200:\n     Train Loss: 1.8036\n     Val  Loss:  2.9143\n     Time:       213.85s\n     LR:         2.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.9143 (√©poca 5)\n\nüìà √âpoca 6/200:\n     Train Loss: 1.7650\n     Val  Loss:  2.9125\n     Time:       213.77s\n     LR:         3.00e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.9143 en la √©poca 5)\n\nüìà √âpoca 7/200:\n     Train Loss: 1.7349\n     Val  Loss:  2.9080\n     Time:       213.38s\n     LR:         3.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.9080 (√©poca 7)\n\nüìà √âpoca 8/200:\n     Train Loss: 1.7104\n     Val  Loss:  2.9039\n     Time:       213.70s\n     LR:         4.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.9039 (√©poca 8)\n\nüìà √âpoca 9/200:\n     Train Loss: 1.6908\n     Val  Loss:  2.8899\n     Time:       212.48s\n     LR:         4.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.8899 (√©poca 9)\n\nüìà √âpoca 10/200:\n     Train Loss: 1.6741\n     Val  Loss:  2.8752\n     Time:       215.42s\n     LR:         5.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.8752 (√©poca 10)\n\nüìà √âpoca 11/200:\n     Train Loss: 1.6595\n     Val  Loss:  2.8557\n     Time:       216.49s\n     LR:         5.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.8557 (√©poca 11)\n\nüìà √âpoca 12/200:\n     Train Loss: 1.6463\n     Val  Loss:  2.8068\n     Time:       212.46s\n     LR:         6.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.8068 (√©poca 12)\n\nüìà √âpoca 13/200:\n     Train Loss: 1.6335\n     Val  Loss:  2.7554\n     Time:       212.52s\n     LR:         6.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.7554 (√©poca 13)\n\nüìà √âpoca 14/200:\n     Train Loss: 1.6217\n     Val  Loss:  2.6757\n     Time:       212.02s\n     LR:         7.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.6757 (√©poca 14)\n\nüìà √âpoca 15/200:\n     Train Loss: 1.6084\n     Val  Loss:  2.5915\n     Time:       211.16s\n     LR:         7.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.5915 (√©poca 15)\n\nüìà √âpoca 16/200:\n     Train Loss: 1.5950\n     Val  Loss:  2.5256\n     Time:       212.44s\n     LR:         8.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.5256 (√©poca 16)\n\nüìà √âpoca 17/200:\n     Train Loss: 1.5812\n     Val  Loss:  2.4868\n     Time:       214.91s\n     LR:         8.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.4868 (√©poca 17)\n\nüìà √âpoca 18/200:\n     Train Loss: 1.5678\n     Val  Loss:  2.5009\n     Time:       213.69s\n     LR:         9.00e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.4868 en la √©poca 17)\n\nüìà √âpoca 19/200:\n     Train Loss: 1.5551\n     Val  Loss:  2.4644\n     Time:       212.85s\n     LR:         9.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.4644 (√©poca 19)\n\nüìà √âpoca 20/200:\n     Train Loss: 1.5434\n     Val  Loss:  2.4862\n     Time:       213.03s\n     LR:         1.00e-04\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.4644 en la √©poca 19)\n\nüìà √âpoca 21/200:\n     Train Loss: 1.5320\n     Val  Loss:  2.4479\n     Time:       211.89s\n     LR:         1.00e-04\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.4479 (√©poca 21)\n\nüìà √âpoca 22/200:\n     Train Loss: 1.5208\n     Val  Loss:  2.4971\n     Time:       210.61s\n     LR:         1.00e-04\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.4479 en la √©poca 21)\n\nüìà √âpoca 23/200:\n     Train Loss: 1.5108\n     Val  Loss:  2.4055\n     Time:       210.64s\n     LR:         9.99e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.4055 (√©poca 23)\n\nüìà √âpoca 24/200:\n     Train Loss: 1.5009\n     Val  Loss:  2.4351\n     Time:       212.37s\n     LR:         9.99e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.4055 en la √©poca 23)\n\nüìà √âpoca 25/200:\n     Train Loss: 1.4914\n     Val  Loss:  2.4121\n     Time:       211.58s\n     LR:         9.98e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.4055 en la √©poca 23)\n\nüìà √âpoca 26/200:\n     Train Loss: 1.4823\n     Val  Loss:  2.3684\n     Time:       211.58s\n     LR:         9.97e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.3684 (√©poca 26)\n\nüìà √âpoca 27/200:\n     Train Loss: 1.4739\n     Val  Loss:  2.3893\n     Time:       211.59s\n     LR:         9.96e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.3684 en la √©poca 26)\n\nüìà √âpoca 28/200:\n     Train Loss: 1.4655\n     Val  Loss:  2.3611\n     Time:       212.03s\n     LR:         9.95e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.3611 (√©poca 28)\n\nüìà √âpoca 29/200:\n     Train Loss: 1.4576\n     Val  Loss:  2.3529\n     Time:       212.69s\n     LR:         9.94e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.3529 (√©poca 29)\n\nüìà √âpoca 30/200:\n     Train Loss: 1.4502\n     Val  Loss:  2.3128\n     Time:       213.35s\n     LR:         9.92e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.3128 (√©poca 30)\n\nüìà √âpoca 31/200:\n     Train Loss: 1.4431\n     Val  Loss:  2.2793\n     Time:       212.72s\n     LR:         9.91e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.2793 (√©poca 31)\n\nüìà √âpoca 32/200:\n     Train Loss: 1.4360\n     Val  Loss:  2.3000\n     Time:       211.02s\n     LR:         9.89e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.2793 en la √©poca 31)\n\nüìà √âpoca 33/200:\n     Train Loss: 1.4293\n     Val  Loss:  2.2422\n     Time:       210.77s\n     LR:         9.87e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.2422 (√©poca 33)\n\nüìà √âpoca 34/200:\n     Train Loss: 1.4229\n     Val  Loss:  2.3444\n     Time:       211.33s\n     LR:         9.85e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.2422 en la √©poca 33)\n\nüìà √âpoca 35/200:\n     Train Loss: 1.4166\n     Val  Loss:  2.2644\n     Time:       211.19s\n     LR:         9.83e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.2422 en la √©poca 33)\n\nüìà √âpoca 36/200:\n     Train Loss: 1.4103\n     Val  Loss:  2.2318\n     Time:       212.99s\n     LR:         9.81e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.2318 (√©poca 36)\n\nüìà √âpoca 37/200:\n     Train Loss: 1.4044\n     Val  Loss:  2.2630\n     Time:       212.49s\n     LR:         9.78e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.2318 en la √©poca 36)\n\nüìà √âpoca 38/200:\n     Train Loss: 1.3988\n     Val  Loss:  2.2389\n     Time:       210.84s\n     LR:         9.76e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.2318 en la √©poca 36)\n\nüìà √âpoca 39/200:\n     Train Loss: 1.3932\n     Val  Loss:  2.2046\n     Time:       213.10s\n     LR:         9.73e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.2046 (√©poca 39)\n\nüìà √âpoca 40/200:\n     Train Loss: 1.3877\n     Val  Loss:  2.2206\n     Time:       214.28s\n     LR:         9.70e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.2046 en la √©poca 39)\n\nüìà √âpoca 41/200:\n     Train Loss: 1.3824\n     Val  Loss:  2.2096\n     Time:       212.78s\n     LR:         9.67e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.2046 en la √©poca 39)\n\nüìà √âpoca 42/200:\n     Train Loss: 1.3774\n     Val  Loss:  2.1618\n     Time:       211.23s\n     LR:         9.64e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.1618 (√©poca 42)\n\nüìà √âpoca 43/200:\n     Train Loss: 1.3724\n     Val  Loss:  2.1402\n     Time:       211.33s\n     LR:         9.61e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.1402 (√©poca 43)\n\nüìà √âpoca 44/200:\n     Train Loss: 1.3676\n     Val  Loss:  2.1780\n     Time:       209.55s\n     LR:         9.57e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.1402 en la √©poca 43)\n\nüìà √âpoca 45/200:\n     Train Loss: 1.3628\n     Val  Loss:  2.1474\n     Time:       210.16s\n     LR:         9.54e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.1402 en la √©poca 43)\n\nüìà √âpoca 46/200:\n     Train Loss: 1.3582\n     Val  Loss:  2.2346\n     Time:       209.88s\n     LR:         9.50e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 2.1402 en la √©poca 43)\n\nüìà √âpoca 47/200:\n     Train Loss: 1.3535\n     Val  Loss:  2.0973\n     Time:       211.48s\n     LR:         9.46e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.0973 (√©poca 47)\n\nüìà √âpoca 48/200:\n     Train Loss: 1.3491\n     Val  Loss:  2.1170\n     Time:       211.99s\n     LR:         9.42e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.0973 en la √©poca 47)\n\nüìà √âpoca 49/200:\n     Train Loss: 1.3447\n     Val  Loss:  2.1211\n     Time:       211.05s\n     LR:         9.38e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.0973 en la √©poca 47)\n\nüìà √âpoca 50/200:\n     Train Loss: 1.3404\n     Val  Loss:  2.1437\n     Time:       211.47s\n     LR:         9.34e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 2.0973 en la √©poca 47)\n\nüìà √âpoca 51/200:\n     Train Loss: 1.3361\n     Val  Loss:  2.0693\n     Time:       211.50s\n     LR:         9.29e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.0693 (√©poca 51)\n\nüìà √âpoca 52/200:\n     Train Loss: 1.3321\n     Val  Loss:  2.0425\n     Time:       213.21s\n     LR:         9.25e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.0425 (√©poca 52)\n\nüìà √âpoca 53/200:\n     Train Loss: 1.3281\n     Val  Loss:  2.0982\n     Time:       213.62s\n     LR:         9.20e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.0425 en la √©poca 52)\n\nüìà √âpoca 54/200:\n     Train Loss: 1.3240\n     Val  Loss:  2.0306\n     Time:       212.25s\n     LR:         9.15e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.0306 (√©poca 54)\n\nüìà √âpoca 55/200:\n     Train Loss: 1.3201\n     Val  Loss:  2.0335\n     Time:       211.73s\n     LR:         9.10e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.0306 en la √©poca 54)\n\nüìà √âpoca 56/200:\n     Train Loss: 1.3162\n     Val  Loss:  2.0436\n     Time:       212.61s\n     LR:         9.05e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.0306 en la √©poca 54)\n\nüìà √âpoca 57/200:\n     Train Loss: 1.3124\n     Val  Loss:  2.0182\n     Time:       211.90s\n     LR:         9.00e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 2.0182 (√©poca 57)\n\nüìà √âpoca 58/200:\n     Train Loss: 1.3086\n     Val  Loss:  2.0204\n     Time:       211.63s\n     LR:         8.95e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 2.0182 en la √©poca 57)\n\nüìà √âpoca 59/200:\n     Train Loss: 1.3050\n     Val  Loss:  2.0467\n     Time:       211.18s\n     LR:         8.90e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 2.0182 en la √©poca 57)\n\nüìà √âpoca 60/200:\n     Train Loss: 1.3013\n     Val  Loss:  1.9847\n     Time:       211.53s\n     LR:         8.84e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.9847 (√©poca 60)\n\nüìà √âpoca 61/200:\n     Train Loss: 1.2977\n     Val  Loss:  2.0345\n     Time:       210.29s\n     LR:         8.79e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.9847 en la √©poca 60)\n\nüìà √âpoca 62/200:\n     Train Loss: 1.2941\n     Val  Loss:  2.0041\n     Time:       213.84s\n     LR:         8.73e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.9847 en la √©poca 60)\n\nüìà √âpoca 63/200:\n     Train Loss: 1.2906\n     Val  Loss:  1.9821\n     Time:       212.59s\n     LR:         8.67e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.9821 (√©poca 63)\n\nüìà √âpoca 64/200:\n     Train Loss: 1.2871\n     Val  Loss:  1.9320\n     Time:       212.24s\n     LR:         8.61e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.9320 (√©poca 64)\n\nüìà √âpoca 65/200:\n     Train Loss: 1.2837\n     Val  Loss:  1.9765\n     Time:       211.40s\n     LR:         8.55e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.9320 en la √©poca 64)\n\nüìà √âpoca 66/200:\n     Train Loss: 1.2803\n     Val  Loss:  1.9654\n     Time:       210.49s\n     LR:         8.49e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.9320 en la √©poca 64)\n\nüìà √âpoca 67/200:\n     Train Loss: 1.2770\n     Val  Loss:  1.9150\n     Time:       212.50s\n     LR:         8.43e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.9150 (√©poca 67)\n\nüìà √âpoca 68/200:\n     Train Loss: 1.2736\n     Val  Loss:  1.8917\n     Time:       213.68s\n     LR:         8.36e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.8917 (√©poca 68)\n\nüìà √âpoca 69/200:\n     Train Loss: 1.2703\n     Val  Loss:  1.9106\n     Time:       212.88s\n     LR:         8.30e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.8917 en la √©poca 68)\n\nüìà √âpoca 70/200:\n     Train Loss: 1.2670\n     Val  Loss:  1.9016\n     Time:       212.93s\n     LR:         8.23e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.8917 en la √©poca 68)\n\nüìà √âpoca 71/200:\n     Train Loss: 1.2638\n     Val  Loss:  1.9034\n     Time:       211.85s\n     LR:         8.17e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.8917 en la √©poca 68)\n\nüìà √âpoca 72/200:\n     Train Loss: 1.2607\n     Val  Loss:  1.8918\n     Time:       212.25s\n     LR:         8.10e-05\n     ‚ö†Ô∏è  Sin mejora por 4 √©poca(s) (mejor val_loss 1.8917 en la √©poca 68)\n\nüìà √âpoca 73/200:\n     Train Loss: 1.2576\n     Val  Loss:  1.8634\n     Time:       212.53s\n     LR:         8.03e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.8634 (√©poca 73)\n\nüìà √âpoca 74/200:\n     Train Loss: 1.2544\n     Val  Loss:  1.8877\n     Time:       209.41s\n     LR:         7.96e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.8634 en la √©poca 73)\n\nüìà √âpoca 75/200:\n     Train Loss: 1.2513\n     Val  Loss:  1.8467\n     Time:       208.61s\n     LR:         7.89e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.8467 (√©poca 75)\n\nüìà √âpoca 76/200:\n     Train Loss: 1.2483\n     Val  Loss:  1.8808\n     Time:       208.12s\n     LR:         7.82e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.8467 en la √©poca 75)\n\nüìà √âpoca 77/200:\n     Train Loss: 1.2452\n     Val  Loss:  1.8226\n     Time:       210.17s\n     LR:         7.75e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.8226 (√©poca 77)\n\nüìà √âpoca 78/200:\n     Train Loss: 1.2421\n     Val  Loss:  1.8239\n     Time:       212.25s\n     LR:         7.67e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.8226 en la √©poca 77)\n\nüìà √âpoca 79/200:\n     Train Loss: 1.2391\n     Val  Loss:  1.7971\n     Time:       211.12s\n     LR:         7.60e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.7971 (√©poca 79)\n\nüìà √âpoca 80/200:\n     Train Loss: 1.2361\n     Val  Loss:  1.8295\n     Time:       208.56s\n     LR:         7.52e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.7971 en la √©poca 79)\n\nüìà √âpoca 81/200:\n     Train Loss: 1.2332\n     Val  Loss:  1.7936\n     Time:       212.98s\n     LR:         7.45e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.7936 (√©poca 81)\n\nüìà √âpoca 82/200:\n     Train Loss: 1.2304\n     Val  Loss:  1.7928\n     Time:       212.95s\n     LR:         7.37e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.7936 en la √©poca 81)\n\nüìà √âpoca 83/200:\n     Train Loss: 1.2274\n     Val  Loss:  1.8025\n     Time:       211.33s\n     LR:         7.30e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.7936 en la √©poca 81)\n\nüìà √âpoca 84/200:\n     Train Loss: 1.2246\n     Val  Loss:  1.7970\n     Time:       213.87s\n     LR:         7.22e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.7936 en la √©poca 81)\n\nüìà √âpoca 85/200:\n     Train Loss: 1.2217\n     Val  Loss:  1.7600\n     Time:       210.95s\n     LR:         7.14e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.7600 (√©poca 85)\n\nüìà √âpoca 86/200:\n     Train Loss: 1.2189\n     Val  Loss:  1.7376\n     Time:       209.80s\n     LR:         7.06e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.7376 (√©poca 86)\n\nüìà √âpoca 87/200:\n     Train Loss: 1.2161\n     Val  Loss:  1.7090\n     Time:       211.19s\n     LR:         6.98e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.7090 (√©poca 87)\n\nüìà √âpoca 88/200:\n     Train Loss: 1.2133\n     Val  Loss:  1.7074\n     Time:       211.20s\n     LR:         6.90e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.7090 en la √©poca 87)\n\nüìà √âpoca 89/200:\n     Train Loss: 1.2106\n     Val  Loss:  1.7852\n     Time:       211.34s\n     LR:         6.82e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.7090 en la √©poca 87)\n\nüìà √âpoca 90/200:\n     Train Loss: 1.2079\n     Val  Loss:  1.7718\n     Time:       210.91s\n     LR:         6.74e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.7090 en la √©poca 87)\n\nüìà √âpoca 91/200:\n     Train Loss: 1.2053\n     Val  Loss:  1.7260\n     Time:       210.49s\n     LR:         6.66e-05\n     ‚ö†Ô∏è  Sin mejora por 4 √©poca(s) (mejor val_loss 1.7090 en la √©poca 87)\n\nüìà √âpoca 92/200:\n     Train Loss: 1.2026\n     Val  Loss:  1.7338\n     Time:       210.93s\n     LR:         6.58e-05\n     ‚ö†Ô∏è  Sin mejora por 5 √©poca(s) (mejor val_loss 1.7090 en la √©poca 87)\n\nüìà √âpoca 93/200:\n     Train Loss: 1.2000\n     Val  Loss:  1.6667\n     Time:       210.78s\n     LR:         6.50e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.6667 (√©poca 93)\n\nüìà √âpoca 94/200:\n     Train Loss: 1.1973\n     Val  Loss:  1.6754\n     Time:       213.02s\n     LR:         6.41e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.6667 en la √©poca 93)\n\nüìà √âpoca 95/200:\n     Train Loss: 1.1947\n     Val  Loss:  1.6667\n     Time:       213.04s\n     LR:         6.33e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.6667 en la √©poca 93)\n\nüìà √âpoca 96/200:\n     Train Loss: 1.1921\n     Val  Loss:  1.7142\n     Time:       211.84s\n     LR:         6.25e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.6667 en la √©poca 93)\n\nüìà √âpoca 97/200:\n     Train Loss: 1.1896\n     Val  Loss:  1.6736\n     Time:       210.25s\n     LR:         6.16e-05\n     ‚ö†Ô∏è  Sin mejora por 4 √©poca(s) (mejor val_loss 1.6667 en la √©poca 93)\n\nüìà √âpoca 98/200:\n     Train Loss: 1.1870\n     Val  Loss:  1.6388\n     Time:       212.27s\n     LR:         6.08e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.6388 (√©poca 98)\n\nüìà √âpoca 99/200:\n     Train Loss: 1.1845\n     Val  Loss:  1.6686\n     Time:       211.36s\n     LR:         5.99e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.6388 en la √©poca 98)\n\nüìà √âpoca 100/200:\n     Train Loss: 1.1820\n     Val  Loss:  1.6554\n     Time:       213.33s\n     LR:         5.91e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.6388 en la √©poca 98)\n\nüìà √âpoca 101/200:\n     Train Loss: 1.1796\n     Val  Loss:  1.6216\n     Time:       213.72s\n     LR:         5.82e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.6216 (√©poca 101)\n\nüìà √âpoca 102/200:\n     Train Loss: 1.1771\n     Val  Loss:  1.6431\n     Time:       215.33s\n     LR:         5.74e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.6216 en la √©poca 101)\n\nüìà √âpoca 103/200:\n     Train Loss: 1.1747\n     Val  Loss:  1.6022\n     Time:       212.75s\n     LR:         5.65e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.6022 (√©poca 103)\n\nüìà √âpoca 104/200:\n     Train Loss: 1.1723\n     Val  Loss:  1.6086\n     Time:       211.46s\n     LR:         5.57e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.6022 en la √©poca 103)\n\nüìà √âpoca 105/200:\n     Train Loss: 1.1699\n     Val  Loss:  1.5874\n     Time:       209.25s\n     LR:         5.48e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5874 (√©poca 105)\n\nüìà √âpoca 106/200:\n     Train Loss: 1.1675\n     Val  Loss:  1.5970\n     Time:       211.47s\n     LR:         5.40e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.5874 en la √©poca 105)\n\nüìà √âpoca 107/200:\n     Train Loss: 1.1651\n     Val  Loss:  1.6124\n     Time:       213.53s\n     LR:         5.31e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.5874 en la √©poca 105)\n\nüìà √âpoca 108/200:\n     Train Loss: 1.1628\n     Val  Loss:  1.5661\n     Time:       212.63s\n     LR:         5.22e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5661 (√©poca 108)\n\nüìà √âpoca 109/200:\n     Train Loss: 1.1604\n     Val  Loss:  1.5945\n     Time:       212.08s\n     LR:         5.14e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.5661 en la √©poca 108)\n\nüìà √âpoca 110/200:\n     Train Loss: 1.1581\n     Val  Loss:  1.6084\n     Time:       213.07s\n     LR:         5.05e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.5661 en la √©poca 108)\n\nüìà √âpoca 111/200:\n     Train Loss: 1.1558\n     Val  Loss:  1.5588\n     Time:       212.06s\n     LR:         4.96e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5588 (√©poca 111)\n\nüìà √âpoca 112/200:\n     Train Loss: 1.1536\n     Val  Loss:  1.5716\n     Time:       211.03s\n     LR:         4.88e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.5588 en la √©poca 111)\n\nüìà √âpoca 113/200:\n     Train Loss: 1.1512\n     Val  Loss:  1.5843\n     Time:       210.40s\n     LR:         4.79e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.5588 en la √©poca 111)\n\nüìà √âpoca 114/200:\n     Train Loss: 1.1490\n     Val  Loss:  1.5365\n     Time:       197.96s\n     LR:         4.70e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5365 (√©poca 114)\n\nüìà √âpoca 115/200:\n     Train Loss: 1.1468\n     Val  Loss:  1.5726\n     Time:       193.25s\n     LR:         4.62e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.5365 en la √©poca 114)\n\nüìà √âpoca 116/200:\n     Train Loss: 1.1446\n     Val  Loss:  1.5244\n     Time:       193.68s\n     LR:         4.53e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5244 (√©poca 116)\n\nüìà √âpoca 117/200:\n     Train Loss: 1.1424\n     Val  Loss:  1.5051\n     Time:       209.02s\n     LR:         4.45e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5051 (√©poca 117)\n\nüìà √âpoca 118/200:\n     Train Loss: 1.1402\n     Val  Loss:  1.5409\n     Time:       201.88s\n     LR:         4.36e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.5051 en la √©poca 117)\n\nüìà √âpoca 119/200:\n     Train Loss: 1.1381\n     Val  Loss:  1.5421\n     Time:       196.60s\n     LR:         4.28e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.5051 en la √©poca 117)\n\nüìà √âpoca 120/200:\n     Train Loss: 1.1360\n     Val  Loss:  1.5034\n     Time:       196.49s\n     LR:         4.19e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.5051 en la √©poca 117)\n\nüìà √âpoca 121/200:\n     Train Loss: 1.1338\n     Val  Loss:  1.5075\n     Time:       199.19s\n     LR:         4.11e-05\n     ‚ö†Ô∏è  Sin mejora por 4 √©poca(s) (mejor val_loss 1.5051 en la √©poca 117)\n\nüìà √âpoca 122/200:\n     Train Loss: 1.1317\n     Val  Loss:  1.5025\n     Time:       198.52s\n     LR:         4.02e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.5025 (√©poca 122)\n\nüìà √âpoca 123/200:\n     Train Loss: 1.1296\n     Val  Loss:  1.4680\n     Time:       198.73s\n     LR:         3.94e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.4680 (√©poca 123)\n\nüìà √âpoca 124/200:\n     Train Loss: 1.1276\n     Val  Loss:  1.4929\n     Time:       197.35s\n     LR:         3.85e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.4680 en la √©poca 123)\n\nüìà √âpoca 125/200:\n     Train Loss: 1.1255\n     Val  Loss:  1.4819\n     Time:       195.81s\n     LR:         3.77e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.4680 en la √©poca 123)\n\nüìà √âpoca 126/200:\n     Train Loss: 1.1235\n     Val  Loss:  1.4760\n     Time:       196.91s\n     LR:         3.69e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.4680 en la √©poca 123)\n\nüìà √âpoca 127/200:\n     Train Loss: 1.1214\n     Val  Loss:  1.4564\n     Time:       195.85s\n     LR:         3.60e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.4564 (√©poca 127)\n\nüìà √âpoca 128/200:\n     Train Loss: 1.1195\n     Val  Loss:  1.5032\n     Time:       197.77s\n     LR:         3.52e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.4564 en la √©poca 127)\n\nüìà √âpoca 129/200:\n     Train Loss: 1.1175\n     Val  Loss:  1.4891\n     Time:       196.94s\n     LR:         3.44e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.4564 en la √©poca 127)\n\nüìà √âpoca 130/200:\n     Train Loss: 1.1155\n     Val  Loss:  1.4502\n     Time:       197.74s\n     LR:         3.36e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.4502 (√©poca 130)\n\nüìà √âpoca 131/200:\n     Train Loss: 1.1135\n     Val  Loss:  1.4355\n     Time:       198.73s\n     LR:         3.28e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.4355 (√©poca 131)\n\nüìà √âpoca 132/200:\n     Train Loss: 1.1116\n     Val  Loss:  1.4752\n     Time:       196.37s\n     LR:         3.20e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 133/200:\n     Train Loss: 1.1096\n     Val  Loss:  1.4482\n     Time:       201.10s\n     LR:         3.12e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 134/200:\n     Train Loss: 1.1077\n     Val  Loss:  1.4988\n     Time:       199.74s\n     LR:         3.04e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 135/200:\n     Train Loss: 1.1058\n     Val  Loss:  1.4828\n     Time:       198.50s\n     LR:         2.96e-05\n     ‚ö†Ô∏è  Sin mejora por 4 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 136/200:\n     Train Loss: 1.1040\n     Val  Loss:  1.4428\n     Time:       201.86s\n     LR:         2.88e-05\n     ‚ö†Ô∏è  Sin mejora por 5 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 137/200:\n     Train Loss: 1.1021\n     Val  Loss:  1.4744\n     Time:       206.22s\n     LR:         2.80e-05\n     ‚ö†Ô∏è  Sin mejora por 6 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 138/200:\n     Train Loss: 1.1003\n     Val  Loss:  1.4398\n     Time:       203.55s\n     LR:         2.73e-05\n     ‚ö†Ô∏è  Sin mejora por 7 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 139/200:\n     Train Loss: 1.0985\n     Val  Loss:  1.4815\n     Time:       197.26s\n     LR:         2.65e-05\n     ‚ö†Ô∏è  Sin mejora por 8 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 140/200:\n     Train Loss: 1.0967\n     Val  Loss:  1.4449\n     Time:       198.05s\n     LR:         2.57e-05\n     ‚ö†Ô∏è  Sin mejora por 9 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 141/200:\n     Train Loss: 1.0949\n     Val  Loss:  1.4517\n     Time:       194.50s\n     LR:         2.50e-05\n     ‚ö†Ô∏è  Sin mejora por 10 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 142/200:\n     Train Loss: 1.0931\n     Val  Loss:  1.4608\n     Time:       196.17s\n     LR:         2.43e-05\n     ‚ö†Ô∏è  Sin mejora por 11 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 143/200:\n     Train Loss: 1.0914\n     Val  Loss:  1.4651\n     Time:       197.15s\n     LR:         2.35e-05\n     ‚ö†Ô∏è  Sin mejora por 12 √©poca(s) (mejor val_loss 1.4355 en la √©poca 131)\n\nüìà √âpoca 144/200:\n     Train Loss: 1.0896\n     Val  Loss:  1.4171\n     Time:       195.58s\n     LR:         2.28e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.4171 (√©poca 144)\n\nüìà √âpoca 145/200:\n     Train Loss: 1.0879\n     Val  Loss:  1.4084\n     Time:       193.64s\n     LR:         2.21e-05\n     üü¢ Mejora en val_loss. Nuevo mejor: 1.4084 (√©poca 145)\n\nüìà √âpoca 146/200:\n     Train Loss: 1.0862\n     Val  Loss:  1.4390\n     Time:       193.40s\n     LR:         2.14e-05\n     ‚ö†Ô∏è  Sin mejora por 1 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 147/200:\n     Train Loss: 1.0845\n     Val  Loss:  1.4380\n     Time:       204.90s\n     LR:         2.07e-05\n     ‚ö†Ô∏è  Sin mejora por 2 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 148/200:\n     Train Loss: 1.0828\n     Val  Loss:  1.4136\n     Time:       201.91s\n     LR:         2.00e-05\n     ‚ö†Ô∏è  Sin mejora por 3 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 149/200:\n     Train Loss: 1.0811\n     Val  Loss:  1.4176\n     Time:       200.21s\n     LR:         1.93e-05\n     ‚ö†Ô∏è  Sin mejora por 4 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 150/200:\n     Train Loss: 1.0795\n     Val  Loss:  1.4084\n     Time:       199.11s\n     LR:         1.87e-05\n     ‚ö†Ô∏è  Sin mejora por 5 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 151/200:\n     Train Loss: 1.0778\n     Val  Loss:  1.4146\n     Time:       199.60s\n     LR:         1.80e-05\n     ‚ö†Ô∏è  Sin mejora por 6 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 152/200:\n     Train Loss: 1.0762\n     Val  Loss:  1.4226\n     Time:       198.61s\n     LR:         1.74e-05\n     ‚ö†Ô∏è  Sin mejora por 7 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 153/200:\n     Train Loss: 1.0745\n     Val  Loss:  1.4153\n     Time:       199.03s\n     LR:         1.67e-05\n     ‚ö†Ô∏è  Sin mejora por 8 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 154/200:\n     Train Loss: 1.0729\n     Val  Loss:  1.4141\n     Time:       195.43s\n     LR:         1.61e-05\n     ‚ö†Ô∏è  Sin mejora por 9 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 155/200:\n     Train Loss: 1.0713\n     Val  Loss:  1.4363\n     Time:       194.19s\n     LR:         1.55e-05\n     ‚ö†Ô∏è  Sin mejora por 10 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 156/200:\n     Train Loss: 1.0698\n     Val  Loss:  1.4384\n     Time:       195.13s\n     LR:         1.49e-05\n     ‚ö†Ô∏è  Sin mejora por 11 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 157/200:\n     Train Loss: 1.0682\n     Val  Loss:  1.4620\n     Time:       194.45s\n     LR:         1.43e-05\n     ‚ö†Ô∏è  Sin mejora por 12 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 158/200:\n     Train Loss: 1.0667\n     Val  Loss:  1.4396\n     Time:       194.55s\n     LR:         1.37e-05\n     ‚ö†Ô∏è  Sin mejora por 13 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 159/200:\n     Train Loss: 1.0651\n     Val  Loss:  1.4210\n     Time:       195.64s\n     LR:         1.31e-05\n     ‚ö†Ô∏è  Sin mejora por 14 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\n\nüìà √âpoca 160/200:\n     Train Loss: 1.0636\n     Val  Loss:  1.4457\n     Time:       194.97s\n     LR:         1.26e-05\n     ‚ö†Ô∏è  Sin mejora por 15 √©poca(s) (mejor val_loss 1.4084 en la √©poca 145)\nüö® EarlyStopping activado: 15 √©pocas sin mejora (paciencia=15).\n    Mejor √©poca: 145 con val_loss=1.4084\n‚úÖ Restaurados pesos de la mejor √©poca: 145 (val_loss=1.4084)\nüíæ Modelo guardado: /kaggle/working/Transformer-B_best.keras\n\n============================================================\nüîç Evaluando modelo: Transformer-B\n============================================================\nEvaluaci√≥n: CER=78.76% | Accuracy=0.00%\nEvaluaci√≥n: CER=77.56% | Accuracy=0.00%\nEvaluaci√≥n: CER=77.28% | Accuracy=0.00%\nEvaluaci√≥n: CER=77.19% | Accuracy=0.00%\nEvaluaci√≥n: CER=78.78% | Accuracy=0.00%\nEvaluaci√≥n: CER=78.72% | Accuracy=0.00%\nEvaluaci√≥n: CER=78.62% | Accuracy=0.00%\nEvaluaci√≥n: CER=78.91% | Accuracy=0.00%\nEvaluaci√≥n: CER=78.73% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.04% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.15% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.27% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.51% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.55% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.54% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.73% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.69% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.54% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.61% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.48% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.66% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.70% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.66% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.78% | Accuracy=0.00%\nEvaluaci√≥n: CER=79.77% | Accuracy=0.00%\n\nüìä Resultados de Transformer-B:\n   CER (Character Error Rate): 79.77%\n   WER (Word Error Rate):      414.51%\n   Accuracy (Exact Match):     0.00%\n   Avg Loss:                   0.0000\n   Avg Time per batch:         195.350s\n   Total samples:              200\n\nüìù Ejemplos de predicciones:\n\n   Ejemplo 1:\n   Real: 'peek out the window'\n   Pred: 'i want to hold your hand'\n\n   Ejemplo 2:\n   Real: 'keep receipts for all your expenses'\n   Pred: 'i want to hold your hand'\n\n   Ejemplo 3:\n   Real: 'the aspirations of a nation'\n   Pred: 'i want to hold your hand'\n\n   Ejemplo 4:\n   Real: 'seasoned golfers love the game'\n   Pred: 'i want to hold your hand'\n\n   Ejemplo 5:\n   Real: 'we dine out on the weekends'\n   Pred: 'i want to hold your hand'\n\n================================================================================\nüìà FASE 3: GENERACI√ìN DE VISUALIZACIONES\n================================================================================\n‚úÖ Gr√°fica guardada: ./asl_results/training_comparison.png\n‚úÖ Gr√°fica guardada: ./asl_results/metrics_comparison.png\n‚úÖ Gr√°fica guardada: ./asl_results/error_analysis.png\n\n================================================================================\nüèÜ RESUMEN FINAL Y CONCLUSIONES\n================================================================================\n\nüìä COMPARACI√ìN DE M√âTRICAS:\n\nM√©trica              Transformer-B  \n-----------------------------------------------------------------\nCER (%)              79.77          \nWER (%)              414.51         \nAccuracy (%)         0.00           \nAvg Loss             0.00           \n\nüíæ ARCHIVOS GENERADOS:\n   üìÅ Directorio: ./asl_results\n   üìä training_comparison.png\n   üìä metrics_comparison.png\n   üìä error_analysis.png\n\n================================================================================\n‚úÖ AN√ÅLISIS COMPLETADO\n================================================================================\n","output_type":"stream"}],"execution_count":21}]}